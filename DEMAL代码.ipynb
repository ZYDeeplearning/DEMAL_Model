{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-29T04:13:45.736003Z","iopub.status.busy":"2024-03-29T04:13:45.735598Z","iopub.status.idle":"2024-03-29T04:14:00.692472Z","shell.execute_reply":"2024-03-29T04:14:00.691223Z","shell.execute_reply.started":"2024-03-29T04:13:45.735976Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting natsort\n","  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n","Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n","Installing collected packages: natsort\n","Successfully installed natsort-8.4.0\n"]}],"source":["!pip install natsort"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-29T04:14:00.695222Z","iopub.status.busy":"2024-03-29T04:14:00.694906Z","iopub.status.idle":"2024-03-29T04:14:07.422468Z","shell.execute_reply":"2024-03-29T04:14:07.421591Z","shell.execute_reply.started":"2024-03-29T04:14:00.695186Z"},"trusted":true},"outputs":[],"source":["#### import os\n","import torch\n","import cv2\n","import random\n","from PIL import Image\n","# import numpy as np\n","from torchvision import transforms\n","import glob as gl\n","from torch.utils import data\n","import natsort\n","#读取完成的图像\n","#training.....\n","class ImagePools(object):\n","    def __init__(self, root='',trans=None,mode=False,high=False):\n","        super().__init__()\n","        self.mode=mode\n","        self.high=high\n","        self.transform = transforms.Compose(trans)\n","        if not self.mode:\n","            \n","            self.A_path = os.path.join(root, \"train_photo/*\")\n","            self.B_path = os.path.join(root, \"hayao/*\")\n","        elif self.high:\n","            self.A_path = os.path.join(root, \"testA/*\")\n","            self.B_path = os.path.join(root, \"testB/*\")\n","        else:    \n","            self.A_path = os.path.join(root, \"train_photo/*\")\n","            self.B_path = os.path.join(root, \"hayao/*\")\n","        # 读取图像文件\n","#         self.list_A = natsort.natsorted(sorted(make_dataset(self.A_path,1000)))\n","#         self.list_B = natsort.natsorted(sorted(make_dataset(self.B_path,1000)))\n","        self.list_A = gl.glob(self.A_path)\n","        self.list_B = gl.glob(self.B_path)\n","\n","    def __getitem__(self, index):\n","        data={}\n","        A_path = self.list_A[index]\n","        B_path = random.choice(self.list_B)\n","        A = Image.open(A_path).convert('RGB')\n","        B = Image.open(B_path).convert('RGB')\n","        A = self.transform(A)\n","        B = self.transform(B)\n","        data.update({\"A\":A,\"B\":B})\n","        return A,B\n","\n","    def __len__(self):\n","        return max(len(self.list_A),len(self.list_B))\n","\n","#testing.....\n","# class ImagePools(object):\n","#     def __init__(self, root='',trans=None,mode=False,high=False):\n","#         super().__init__()\n","#         self.mode=mode\n","#         self.high=high\n","#         self.transform = transforms.Compose(trans)\n","#         if not self.mode:\n","#             self.A_path = os.path.join(root, \"testHR\")\n","#             self.B_path = os.path.join(root, \"hayao\")\n","            \n","# #             self.A_path = os.path.join(root, \"train_photo\")\n","# #             self.B_path = os.path.join(root, \"hayao\")\n","# #         elif self.high:\n","# #             self.A_path = os.path.join(root, \"testA/*\")\n","# #             self.B_path = os.path.join(root, \"testB/*\")\n","        \n","#         # 读取图像文件\n","#         self.list_A = natsort.natsorted(sorted(make_dataset(self.A_path,1000)))\n","#         self.list_B = natsort.natsorted(sorted(make_dataset(self.B_path,1000)))\n","# #         self.list_A = gl.glob(self.A_path)\n","# #         self.list_B = gl.glob(self.B_path)\n","\n","#     def __getitem__(self, index):\n","#         data={}\n","#         A_path = self.list_A[index]\n","#         B_path = random.choice(self.list_B)\n","#         A = Image.open(A_path).convert('RGB')\n","#         B = Image.open(B_path).convert('RGB')\n","#         A = self.transform(A)\n","#         B = self.transform(B)\n","#         data.update({\"A\":A,\"B\":B})\n","#         return A,B\n","\n","#     def __len__(self):\n","# #         return max(len(self.list_A),len(self.list_B))\n","#         return min(len(self.list_A),len(self.list_B))\n","\n","IMG_EXTENSIONS = [\n","    '.jpg', '.JPG', '.jpeg', '.JPEG',\n","    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n","    '.tif', '.TIF', '.tiff', '.TIFF',\n","]\n","\n","def is_image_file(filename):\n","    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n","\n","def make_dataset(dir, max_dataset_size=float(\"inf\")):\n","    images = []\n","    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n","    for root, _, fnames in sorted(os.walk(dir)):\n","        for fname in fnames:\n","            if is_image_file(fname):\n","                path = os.path.join(root, fname)\n","                images.append(path)\n","    return images[:min(max_dataset_size, len(images))]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-29T04:14:07.424464Z","iopub.status.busy":"2024-03-29T04:14:07.424049Z","iopub.status.idle":"2024-03-29T04:14:07.513599Z","shell.execute_reply":"2024-03-29T04:14:07.512685Z","shell.execute_reply.started":"2024-03-29T04:14:07.424437Z"},"tags":[],"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import numpy as np\n","import cv2\n","import os\n","from torch.autograd import Variable\n","\n","   \n","#参数权重初始化\n","def init_weights(m, init_type='normal', gain=0.02):\n","    classname = m.__class__.__name__\n","    if classname.find('BatchNorm2d') != -1:\n","        if hasattr(m, 'weight') and m.weight is not None:\n","            torch.nn.init.normal_(m.weight.data, 1.0, gain)\n","        if hasattr(m, 'bias') and m.bias is not None:\n","            torch.nn.init.constant_(m.bias.data, 0.0)\n","    elif hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n","        if init_type == 'normal':\n","            torch.nn.init.normal_(m.weight.data, 0.0, gain)\n","        elif init_type == 'xavier':\n","            torch.nn.init.xavier_normal_(m.weight.data, gain=gain)\n","        elif init_type == 'xavier_uniform':\n","            torch.nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n","        elif init_type == 'kaiming':\n","            torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","        elif init_type == 'orthogonal':\n","            torch.nn.init.orthogonal_(m.weight.data, gain=gain)\n","        elif init_type == 'none':  # uses pytorch's default init method\n","            m.reset_parameters()\n","        else:\n","            raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n","        if hasattr(m, 'bias') and m.bias is not None:\n","            torch.nn.init.constant_(m.bias.data, 0.0)\n","#梯度\n","def requires_grad(model, flag=True):\n","    if model is None:\n","        return\n","    for p in model.parameters():\n","        p.requires_grad = flag\n","\n","def rgb_to_yuv(image,x):\n","    image = (image + 1.0) / 2.0\n","    yuv_img = torch.tensordot(\n","        image,\n","        x,\n","        dims=([image.ndim - 3], [0]))\n","    return yuv_img\n","#时间转化\n","def time_change(time):\n","    new_time = t.localtime(time)\n","    new_time = t.strftime(\"%Hh%Mm%Ss\", new_time)\n","    return new_time\n","#归一化\n","def  denorm(x):\n","    x=(x* 0.5+ 0.5)*255.0\n","    return x.cpu().detach().numpy().transpose(1,2,0)\n","def  process(x):\n","    x=(x* 0.5+ 0.5)*255.0\n","    return x.cpu().detach().numpy()[0][0]\n","def RGB2BGR(x):\n","    return cv2.cvtColor(x, cv2.COLOR_RGB2BGR)\n","#创建文件目录\n","def check_folder(log_dir):\n","    if not os.path.exists(log_dir):\n","        os.makedirs(log_dir)\n","    return log_dir\n","#颜色RGB->Gray(DCT-NET)\n","def color_shift(image, mode='uniform'):\n","    device = image.device\n","    b1, g1, r1 = torch.split(image, 1, dim=1)\n","    if mode == 'normal':\n","        b_weight = torch.normal(mean=0.114, std=0.1, size=[1]).to(device)\n","        g_weight = torch.normal(mean=0.587, std=0.1, size=[1]).to(device)\n","        r_weight = torch.normal(mean=0.299, std=0.1, size=[1]).to(device)\n","    elif mode == 'uniform':\n","        b_weight = torch.FloatTensor(1).uniform_(0.014, 0.214).to(device)\n","        g_weight = torch.FloatTensor(1).uniform_(0.487, 0.687).to(device)\n","        r_weight = torch.FloatTensor(1).uniform_(0.199, 0.399).to(device)\n","    output1 = (b_weight * b1 + g_weight * g1 + r_weight * r1) / (b_weight + g_weight + r_weight)\n","    return output1\n","#图像pool\n","class Queue():\n","    def __init__(self, pool_size=50):\n","        self.pool_size = pool_size\n","        if self.pool_size > 0:\n","            self.num_imgs = 0\n","            self.images = []\n","\n","    def query(self, images):\n","        if self.pool_size == 0:\n","            return images\n","        return_images = []\n","        for image in images.data:\n","            image = torch.unsqueeze(image, 0)\n","            if self.num_imgs < self.pool_size:\n","                self.num_imgs = self.num_imgs + 1\n","                self.images.append(image)\n","                return_images.append(image)\n","            else:\n","                p = random.uniform(0, 1)\n","                if p > 0.5:\n","                    random_id = random.randint(0, self.pool_size-1)\n","                    tmp = self.images[random_id].clone()\n","                    self.images[random_id] = image\n","                    return_images.append(tmp)\n","                else:\n","                    return_images.append(image)\n","        return_images = Variable(torch.cat(return_images, 0))\n","        return return_images\n","#颜色white-box\n","class ColorShift():\n","    def __init__(self, device: torch.device = 'cuda', mode='uniform', image_format='rgb'):\n","        self.dist: torch.distributions = None\n","        self.dist_param1: torch.Tensor = None\n","        self.dist_param2: torch.Tensor = None\n","\n","        if (mode == 'uniform'):\n","            self.dist_param1 = torch.tensor((0.199, 0.487, 0.014), device=device)\n","            self.dist_param2 = torch.tensor((0.399, 0.687, 0.214), device=device)\n","            if (image_format == 'bgr'):\n","                self.dist_param1 = torch.permute(self.dist_param1, (2, 1, 0))\n","                self.dist_param2 = torch.permute(self.dist_param2, (2, 1, 0))\n","\n","            self.dist = torch.distributions.Uniform(low=self.dist_param1, high=self.dist_param2)\n","\n","\n","        elif (mode == 'normal'):\n","            self.dist_param1 = torch.tensor((0.299, 0.587, 0.114), device=device)\n","            self.dist_param2 = torch.tensor((0.1, 0.1, 0.1), device=device)\n","            if (image_format == 'bgr'):\n","                self.dist_param1 = torch.permute(self.dist_param1, (2, 1, 0))\n","                self.dist_param2 = torch.permute(self.dist_param2, (2, 1, 0))\n","\n","            self.dist = torch.distributions.Normal(loc=self.dist_param1, scale=self.dist_param2)\n","\n","    # Allow taking mutiple images batches as input\n","    # So we can do: gray_fake, gray_cartoon = ColorShift(output, input_cartoon)\n","    def process(self, *image_batches: torch.Tensor):\n","        # Sample the random color shift coefficients\n","        weights = self.dist.sample()\n","\n","        # images * weights[None, :, None, None] => Apply weights to r,g,b channels of each images\n","        # torch.sum(, dim=1) => Sum along the channels so (B, 3, H, W) become (B, H, W)\n","        # .unsqueeze(1) => add back the channel so (B, H, W) become (B, 1, H, W)\n","        # .repeat(1, 3, 1, 1) => (B, 1, H, W) become (B, 3, H, W) again\n","        return (\n","        (((torch.sum(images * weights[None, :, None, None], dim=1)) / weights.sum()).unsqueeze(1)).repeat(1, 3, 1, 1)\n","        for images in image_batches)\n","\n","#patch 抽取\n","def extract_image_patches(x, kernel, stride):\n","\n","        if kernel != 1:\n","            x = nn.ZeroPad2d(1)(x)\n","        x = x.permute(0, 2, 3, 1)\n","        all_patches = x.unfold(1, kernel, stride).unfold(2, kernel, stride)\n","        all_patches = all_patches.permute(0, 3, 1, 2, 4, 5)\n","        all_patches = all_patches.reshape([all_patches.shape[0] * all_patches.shape[2] ** 2, all_patches.shape[1],\n","                                           all_patches.shape[4], all_patches.shape[5]])\n","        return all_patches\n","def gram(input):\n","    b, c, w, h = input.size()\n","    x = input.view(b * c, w * h)\n","    G = torch.mm(x, x.T)\n","    return G.div(b * c * w * h)\n","\n","def print_network(net):\n","    num_params = 0\n","    for param in net.parameters():\n","        num_params += param.numel()\n","    return num_params\n","def high_pass_filter(img, d, n):\n","    return (1 - 1 / (1 + (img / d) ** n))\n","\n","#引导滤波\n","class GuidedFilter():\n","    def box_filter(self, x, r):\n","        channel =  x.shape[1] # Batch, Channel, H, W\n","        kernel_size = (2*r+1)\n","        weight = 1.0/(kernel_size**2)\n","        box_kernel = weight*torch.ones((channel, 1, kernel_size, kernel_size), dtype=torch.float32, device=x.device)\n","        output = F.conv2d(x, weight=box_kernel, stride=1, padding=r, groups=channel) #tf.nn.depthwise_conv2d(x, box_kernel, [1, 1, 1, 1], 'SAME')\n","        return output\n","    def guided_filter(self, x, y, r, eps=1e-2):\n","        # Batch, Channel, H, W\n","        _, _, H, W = x.shape\n","\n","        N = self.box_filter(torch.ones((1, 1, H, W), dtype=x.dtype, device=x.device), r)\n","\n","        mean_x = self.box_filter(x, r) / N\n","        mean_y = self.box_filter(y, r) / N\n","        cov_xy = self.box_filter(x * y, r) / N - mean_x * mean_y\n","        var_x  = self.box_filter(x * x, r) / N - mean_x * mean_x\n","\n","        A = cov_xy / (var_x + eps)\n","        b = mean_y - A * mean_x\n","\n","        mean_A = self.box_filter(A, r) / N\n","        mean_b = self.box_filter(b, r) / N\n","\n","        output = mean_A * x + mean_b\n","        return output\n","\n","#tv_loss\n","class VariationLoss(nn.Module):\n","    def __init__(self, k_size: int) -> None:\n","        super().__init__()\n","        self.k_size = k_size\n","\n","    def forward(self, image: torch.Tensor):\n","        b, c, h, w = image.shape\n","        tv_h = torch.mean((image[:, :, self.k_size:, :] - image[:, :, : -self.k_size, :])**2)\n","        tv_w = torch.mean((image[:, :, :, self.k_size:] - image[:, :, :, : -self.k_size])**2)\n","        tv_loss = (tv_h + tv_w) / (3 * h * w)\n","        return tv_loss\n","\n","def get_gaussian_kernel(k=3, mu=0, sigma=1, normalize=True):\n","   # compute 1 dimension gaussian\n","   gaussian_1D = np.linspace(-1, 1, k)\n","   # compute a grid distance from center\n","   x, y = np.meshgrid(gaussian_1D, gaussian_1D)\n","   distance = (x ** 2 + y ** 2) ** 0.5\n","\n","   # compute the 2 dimension gaussian\n","   gaussian_2D = np.exp(-(distance - mu) ** 2 / (2 * sigma ** 2))\n","   gaussian_2D = gaussian_2D / (2 * np.pi * sigma ** 2)\n","\n","   # normalize part (mathematically)\n","   if normalize:\n","       gaussian_2D = gaussian_2D / np.sum(gaussian_2D)\n","   return gaussian_2D\n","\n","\n","def get_sobel_kernel(k=3):\n","   # get range\n","   range = np.linspace(-(k // 2), k // 2, k)\n","   # compute a grid the numerator and the axis-distances\n","   x, y = np.meshgrid(range, range)\n","   sobel_2D_numerator = x\n","   sobel_2D_denominator = (x ** 2 + y ** 2)\n","   sobel_2D_denominator[:, k // 2] = 1  # avoid division by zero\n","   sobel_2D = sobel_2D_numerator / sobel_2D_denominator\n","   return sobel_2D\n","\n","\n","def get_thin_kernels(start=0, end=360, step=45):\n","   k_thin = 3  # actual size of the directional kernel\n","   # increase for a while to avoid interpolation when rotating\n","   k_increased = k_thin + 2\n","\n","   # get 0° angle directional kernel\n","   thin_kernel_0 = np.zeros((k_increased, k_increased))\n","   thin_kernel_0[k_increased // 2, k_increased // 2] = 1\n","   thin_kernel_0[k_increased // 2, k_increased // 2 + 1:] = -1\n","\n","   # rotate the 0° angle directional kernel to get the other ones\n","   thin_kernels = []\n","   for angle in range(start, end, step):\n","       (h, w) = thin_kernel_0.shape\n","       # get the center to not rotate around the (0, 0) coord point\n","       center = (w // 2, h // 2)\n","       # apply rotation\n","       rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1)\n","       kernel_angle_increased = cv2.warpAffine(thin_kernel_0, rotation_matrix, (w, h), cv2.INTER_NEAREST)\n","\n","       # get the k=3 kerne\n","       kernel_angle = kernel_angle_increased[1:-1, 1:-1]\n","       is_diag = (abs(kernel_angle) == 1)  # because of the interpolation\n","       kernel_angle = kernel_angle * is_diag  # because of the interpolation\n","       thin_kernels.append(kernel_angle)\n","   return thin_kernels\n","\n","#Canny 边缘检测\n","class CannyFilter(nn.Module):\n","   def __init__(self,\n","                k_gaussian=3,\n","                mu=0,\n","                sigma=1,\n","                k_sobel=3,\n","                device = 'cuda:0'):\n","       super(CannyFilter, self).__init__()\n","       # device\n","       self.device = device\n","       # gaussian\n","       gaussian_2D = get_gaussian_kernel(k_gaussian, mu, sigma)\n","       self.gaussian_filter = nn.Conv2d(in_channels=1,\n","                                        out_channels=1,\n","                                        kernel_size=k_gaussian,\n","                                        padding=k_gaussian // 2,\n","                                        bias=False)\n","       self.gaussian_filter.weight.data[:,:] = nn.Parameter(torch.from_numpy(gaussian_2D), requires_grad=False)\n","\n","       # sobel\n","\n","       sobel_2D = get_sobel_kernel(k_sobel)\n","       self.sobel_filter_x = nn.Conv2d(in_channels=1,\n","                                       out_channels=1,\n","                                       kernel_size=k_sobel,\n","                                       padding=k_sobel // 2,\n","                                       bias=False)\n","       self.sobel_filter_x.weight.data[:,:] = nn.Parameter(torch.from_numpy(sobel_2D), requires_grad=False)\n","\n","       self.sobel_filter_y = nn.Conv2d(in_channels=1,\n","                                       out_channels=1,\n","                                       kernel_size=k_sobel,\n","                                       padding=k_sobel // 2,\n","                                       bias=False)\n","       self.sobel_filter_y.weight.data[:,:] = nn.Parameter(torch.from_numpy(sobel_2D.T), requires_grad=False)\n","\n","       # thin\n","\n","       thin_kernels = get_thin_kernels()\n","       directional_kernels = np.stack(thin_kernels)\n","\n","       self.directional_filter = nn.Conv2d(in_channels=1,\n","                                           out_channels=8,\n","                                           kernel_size=thin_kernels[0].shape,\n","                                           padding=thin_kernels[0].shape[-1] // 2,\n","                                           bias=False)\n","       self.directional_filter.weight.data[:, 0] = nn.Parameter(torch.from_numpy(directional_kernels), requires_grad=False)\n","\n","       # hysteresis\n","\n","       hysteresis = np.ones((3, 3)) + 0.25\n","       self.hysteresis = nn.Conv2d(in_channels=1,\n","                                   out_channels=1,\n","                                   kernel_size=3,\n","                                   padding=1,\n","                                   bias=False)\n","       self.hysteresis.weight.data[:,:] = nn.Parameter(torch.from_numpy(hysteresis), requires_grad=False)\n","\n","   def forward(self, img, low_threshold=None, high_threshold=None, hysteresis=True):\n","       # set the setps tensors\n","       B, C, H, W = img.shape\n","       blurred = torch.zeros((B, C, H, W)).to(self.device)\n","       grad_x = torch.zeros((B, 1, H, W)).to(self.device)\n","       grad_y = torch.zeros((B, 1, H, W)).to(self.device)\n","       grad_magnitude = torch.zeros((B, 1, H, W)).to(self.device)\n","       grad_orientation = torch.zeros((B, 1, H, W)).to(self.device)\n","\n","       # gaussian\n","\n","       for c in range(C):\n","           blurred[:, c:c + 1] = self.gaussian_filter(img[:, c:c + 1])\n","           grad_x = grad_x + self.sobel_filter_x(blurred[:, c:c + 1])\n","           grad_y = grad_y + self.sobel_filter_y(blurred[:, c:c + 1])\n","\n","       # thick edges\n","\n","       grad_x, grad_y = grad_x / C, grad_y / C\n","       grad_magnitude = (grad_x ** 2 + grad_y ** 2) ** 0.5\n","       grad_orientation = torch.atan2(grad_y, grad_x)\n","       grad_orientation = grad_orientation * (180 / np.pi) + 180  # convert to degree\n","       grad_orientation = torch.round(grad_orientation / 45) * 45  # keep a split by 45\n","\n","       # thin edges\n","\n","       directional = self.directional_filter(grad_magnitude)\n","       # get indices of positive and negative directions\n","       positive_idx = (grad_orientation / 45) % 8\n","       negative_idx = ((grad_orientation / 45) + 4) % 8\n","       thin_edges = grad_magnitude.clone()\n","       # non maximum suppression direction by direction\n","       for pos_i in range(4):\n","           neg_i = pos_i + 4\n","           # get the oriented grad for the angle\n","           is_oriented_i = (positive_idx == pos_i) * 1\n","           is_oriented_i = is_oriented_i + (positive_idx == neg_i) * 1\n","           pos_directional = directional[:, pos_i]\n","           neg_directional = directional[:, neg_i]\n","           selected_direction = torch.stack([pos_directional, neg_directional])\n","\n","           # get the local maximum pixels for the angle\n","           # selected_direction.min(dim=0)返回一个列表[0]中包含两者中的小的，[1]包含了小值的索引\n","           is_max = selected_direction.min(dim=0)[0] > 0.0\n","           is_max = torch.unsqueeze(is_max, dim=1)\n","\n","           # apply non maximum suppression\n","           to_remove = (is_max == 0) * 1 * (is_oriented_i) > 0\n","           thin_edges[to_remove] = 0.0\n","\n","       # thresholds\n","\n","       if low_threshold is not None:\n","           low = thin_edges > low_threshold\n","\n","           if high_threshold is not None:\n","               high = thin_edges > high_threshold\n","               # get black/gray/white only\n","               thin_edges = low * 0.5 + high * 0.5\n","\n","               if hysteresis:\n","                   # get weaks and check if they are high or not\n","                   weak = (thin_edges == 0.5) * 1\n","                   weak_is_high = (self.hysteresis(thin_edges) > 1) * weak\n","                   thin_edges = high * 1 + weak_is_high * 1\n","           else:\n","               thin_edges = low * 1\n","\n","       return thin_edges * 255\n","#掩码操作mask\n","def mask(img,mask,y1,y2,Canny):\n","    edge_fake_img = Canny(img, y1, y2)\n","    edge_real_img = Canny(mask, y1, y2)\n","    edge_fake_img=high_pass_filter(edge_fake_img,d=0.2, n=2)\n","    edge_real_img=high_pass_filter(edge_real_img,d=0.2, n=2)\n","    #转换\n","    edge_real_img=process(edge_real_img)\n","    edge_fake_img = process(edge_fake_img)\n","    result=cv2.bitwise_and( edge_fake_img,  edge_real_img)\n","    result=torch.from_numpy(result)\n","    edge_real_img=torch.from_numpy(edge_real_img)\n","    return result,edge_real_img\n","#计算均值\n","def calc_mean_std(feat, eps=1e-5):\n","    # eps is a small value added to the variance to avoid divide-by-zero.\n","    size = feat.size()\n","    assert (len(size) == 4)\n","    N, C = size[:2]\n","    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n","    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n","    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n","    return feat_mean, feat_std\n","\n","def calc_mean(feat):\n","    size = feat.size()\n","    assert (len(size) == 4)\n","    N, C = size[:2]\n","    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n","    return feat_mean\n","\n","def nor_mean_std(feat):\n","    size = feat.size()\n","    mean, std = calc_mean_std(feat)\n","    nor_feat = (feat - mean.expand(size)) / std.expand(size)\n","    return nor_feat\n","\n","def nor_mean(feat):\n","    size = feat.size()\n","    mean = calc_mean(feat)\n","    nor_feat = feat - mean.expand(size)\n","    return nor_feat, mean    \n","\n","def calc_cov(feat):\n","    feat = feat.flatten(2, 3)\n","    f_cov = torch.bmm(feat, feat.permute(0,2,1)).div(feat.size(2))\n","    return f_cov "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-29T04:14:07.518545Z","iopub.status.busy":"2024-03-29T04:14:07.518221Z","iopub.status.idle":"2024-03-29T04:14:07.601144Z","shell.execute_reply":"2024-03-29T04:14:07.600061Z","shell.execute_reply.started":"2024-03-29T04:14:07.518510Z"},"tags":[],"trusted":true},"outputs":[],"source":["import cv2\n","import torch\n","from torch import nn\n","from torch.nn import Parameter\n","from torch.nn import functional as F\n","from torch.nn.utils import spectral_norm\n","from torchvision import transforms\n","# from torchvision.transforms import InterpolationMode\n","from torch.utils import data\n","#channle_wise\n","class layer_norm(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return F.layer_norm(x, x.size()[1:])\n","class Conv2D(nn.Module):\n","    def __init__(self,in_channels=256,out_channels=256, kernel_size=3, strides=1,padding=1,sn=False):\n","        super().__init__()\n","        self.sn=sn\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=strides, padding=padding,\n","                     padding_mode=\"reflect\")\n","\n","    def forward(self,x):\n","        return self.conv(x)\n","#定义卷积\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, strides=1, padding=0,sn=False, bias=False,padding_mode='reflect'):\n","        super(ConvBlock, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels,\n","            kernel_size=kernel_size, stride=strides, padding=padding, bias=bias,padding_mode=padding_mode)\n","        self.ins_norm= layer_norm()\n","        self.activation = nn.LeakyReLU(0.2, True)\n","\n","        init_weights(self)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.ins_norm(out)\n","        out = self.activation(out)\n","\n","        return out\n","    \n","class Discriminator_T(nn.Module):\n","    def __init__(self):\n","        super( Discriminator_T,self).__init__()\n","        self.channels=[1,32,64,128,256]\n","        self.model=[]\n","        self.model+=[ConvBlock(in_channels=self.channels[0],out_channels=self.channels[1],padding=1,sn=True),\n","                     ConvBlock(in_channels=self.channels[1],out_channels=self.channels[2],strides=2,padding=1,sn=True),\n","                     ConvBlock(in_channels=self.channels[2],out_channels=self.channels[3],strides=2,padding=1,sn=True),\n","                     ConvBlock(in_channels=self.channels[3],out_channels=self.channels[4],strides=2,padding=1,sn=True),\n","                     Conv2D(in_channels=self.channels[4],out_channels=1,padding=1,sn=True)]\n","        self.model=nn.Sequential(*self.model)\n","    def forward(self,x):\n","        #3*1\n","        out=self.model(x)\n","        return out\n","#定义生成器\n","class ILN(nn.Module):\n","    def __init__(self, num_features, eps=1e-5):\n","        super(ILN, self).__init__()\n","        self.eps = eps\n","        self.rho = Parameter(torch.Tensor(1, num_features, 1, 1))\n","        self.gamma = Parameter(torch.Tensor(1, num_features, 1, 1))\n","        self.beta = Parameter(torch.Tensor(1, num_features, 1, 1))\n","        self.rho.data.fill_(0.0)\n","        self.gamma.data.fill_(1.0)\n","        self.beta.data.fill_(0.0)\n","\n","    def forward(self, input):\n","        in_mean, in_var = torch.mean(input, dim=[2, 3], keepdim=True), torch.var(input, dim=[2, 3], keepdim=True)\n","        out_in = (input - in_mean) / torch.sqrt(in_var + self.eps)\n","        ln_mean, ln_var = torch.mean(input, dim=[1, 2, 3], keepdim=True), torch.var(input, dim=[1, 2, 3], keepdim=True)\n","        out_ln = (input - ln_mean) / torch.sqrt(ln_var + self.eps)\n","        out = self.rho.expand(input.shape[0], -1, -1, -1) * out_in + (1-self.rho.expand(input.shape[0], -1, -1, -1)) * out_ln\n","        out = out * self.gamma.expand(input.shape[0], -1, -1, -1) + self.beta.expand(input.shape[0], -1, -1, -1)\n","        return out\n","#残差的SCT\n","class utm(nn.Module):\n","    def __init__(self):\n","        super(utm, self).__init__()\n","\n","        self.net = nn.Sequential(nn.Conv2d(256,128,1,1,0),\n","                nn.ReLU(inplace=True),\n","                nn.Conv2d(128,32,1,1,0))\n","        self.uncompress = nn.Conv2d(32,256,1,1,0)  \n","        self.sm = nn.Softmax(dim=-1)\n","    def forward(self, content, style=None,noise=None,init=False):\n","        if init:\n","            cF_nor = nor_mean_std(content)\n","            cF = self.net(cF_nor)\n","            cF = self.uncompress(cF)\n","            cF = cF +content\n","            return cF\n","        else:\n","            cF_nor = nor_mean_std(content)\n","            sF_nor, smean = nor_mean(style)\n","            cF = self.net(cF_nor)\n","            sF = self.net(sF_nor)\n","            b, c, w, h = cF.size()\n","            s_cov = calc_cov(sF)\n","            b1, c1, hw = s_cov.size()\n","            s_cov = self.sm(s_cov) * int(c1) ** (-0.5)#test\n","            gF = torch.bmm(s_cov, cF.flatten(2, 3)).view(b,c,w,h)\n","            gF = self.uncompress(gF)\n","            if noise==None:\n","                gF = gF + smean.expand(cF_nor.size())+content\n","            else:\n","                gF = gF + smean.expand(cF_nor.size())+content\n","            return gF\n","# 定义残差块\n","class ResBlock(nn.Module):\n","    def __init__(self, channels, use_bias=False):\n","        super().__init__()\n","        Res_block = []\n","        Res_block += [nn.ReflectionPad2d(1),\n","                      nn.Conv2d(channels, channels, 3, 1, 0, bias=use_bias),\n","                      ILN(channels), nn.PReLU(num_parameters=1)]\n","\n","        Res_block += [nn.ReflectionPad2d(1),\n","                      nn.Conv2d(channels, channels, 3, 1, 0, bias=use_bias),\n","                      ILN(channels)]\n","        self.Res_block = nn.Sequential(*Res_block)\n","\n","    def forward(self, x):\n","        return x + self.Res_block(x)\n","\n","class StyleEncoder(nn.Module):\n","    def __init__(self, img_channels=3, num_features=64, padding_mode=\"reflect\",):\n","        super().__init__()\n","        self.padding_mode = padding_mode\n","\n","        self.initial_down = nn.Sequential(\n","            # k7n32s1\n","            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3,\n","                      padding_mode=self.padding_mode),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","        )\n","\n","        # Down-convolution\n","        self.down1 = nn.Sequential(\n","            # k3n32s2\n","            nn.Conv2d(num_features, num_features, kernel_size=3, stride=2, padding=1,\n","                      padding_mode=self.padding_mode),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","\n","            # k3n64s1\n","            nn.Conv2d(num_features, num_features * 2, kernel_size=3, stride=1, padding=1,\n","                      padding_mode=self.padding_mode),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","        )\n","\n","        self.down2 = nn.Sequential(\n","            # k3n64s2\n","            nn.Conv2d(num_features * 2, num_features * 2, kernel_size=3, stride=2, padding=1,\n","                      padding_mode=self.padding_mode),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","\n","            # k3n128s1\n","            nn.Conv2d(num_features * 2, num_features * 4, kernel_size=3, stride=1, padding=1,\n","                      padding_mode=self.padding_mode),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","           )\n","    def forward(self, x):\n","        style_code= self.down2(self.down1(self.initial_down(x)))\n","        \n","        return  style_code\n","# 定义生成器-Encode\n","class GenEncoder(nn.Module):\n","    def __init__(self, hw=64, n_block=None,norm=nn.InstanceNorm2d, use_bias=False):\n","        super().__init__()\n","        # 平面卷积\n","        model = []\n","        model += [nn.ReflectionPad2d(3),\n","                  nn.Conv2d(3, hw, 7, 1, 0, bias=use_bias)]\n","        # 下采样\n","        down = 2\n","        for i in range(down):\n","            mult = 2 ** i\n","            model += [nn.Conv2d(hw * mult, hw * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n","                     ILN(hw * mult * 2), nn.ReLU(True)]\n","            # 残差块\n","        res = hw * 4\n","        for j in range(n_block):\n","            model += [ResBlock(res)]\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        return self.model(x)  # 1,256,64,64\n","\n","\n","# 定义解码器\n","class GenDecoder(nn.Module):\n","    def __init__(self, hw=64, out_channels=3, n_block=None, use_bias=False):\n","        super().__init__()\n","        # 残差块\n","        model = []\n","        res = hw * 4\n","        for i in range(n_block):\n","            model += [ResBlock(res)]\n","        # frist upsample\n","        mult = 2 ** (n_block // 2)\n","        model += [nn.Upsample(scale_factor=2, mode='bilinear',align_corners=True),\n","                  nn.Conv2d(int(hw * mult), int(hw * mult / 2), kernel_size=3, stride=1, padding=1),\n","                  nn.Conv2d(int(hw * mult / 2), int(hw * mult / 2), kernel_size=3, stride=1, padding=1),\n","                  ILN(int(hw * mult / 2)),\n","                  nn.ReLU(True)]\n","        # second upsampling\n","        model += [nn.Upsample(scale_factor=2, mode='bilinear',align_corners=True),\n","                  nn.Conv2d(int(hw * mult / 2), int(hw * mult / 4), kernel_size=3, stride=1, padding=1),\n","                  nn.Conv2d(int(hw * mult / 4), int(hw * mult / 4), kernel_size=3, stride=1, padding=1),\n","                  ILN(int(hw * mult / 4)),\n","                  nn.ReLU(True)]\n","        # addtional layer\n","        model += [nn.Conv2d(int(hw * mult / 4), int(hw * mult / 8), kernel_size=3, stride=1, padding=1),\n","                  nn.Conv2d(int(hw * mult / 8), int(hw * mult / 8), kernel_size=3, stride=1, padding=1),\n","                  ILN(int(hw * mult / 8)), nn.ReLU(True)]\n","        model += [nn.Conv2d(int(hw * mult / 8), int(hw * mult / 16), kernel_size=3, stride=1, padding=1),\n","                  nn.Conv2d(int(hw * mult / 16), int(hw * mult / 16), kernel_size=3, stride=1, padding=1),\n","                  ILN(int(hw * mult / 16)), nn.ReLU(True)]\n","        model += [nn.Conv2d(int(hw * mult / 16), out_channels, 7, 1, 3), nn.Tanh()]\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","    # 定义生成器\n","\n","\n","#定义生成器               \n","#定义生成器\n","class Generator(nn.Module):\n","        def __init__(self,n_domian=None,E_block=5,D_block=4):\n","                  super(Generator,self).__init__()\n","                  #编码器\n","                  self.Encoder=[GenEncoder(n_block=E_block)]\n","                  self.Encoder=nn.Sequential(*self.Encoder)\n","                  #解码器\n","                  self.Decoder=[GenDecoder(n_block=D_block)]\n","                  self.Decoder=nn.Sequential(*self.Decoder)\n","        def encoder(self,x):\n","                    return self.Encoder(x)  # type: ignore\n","        def decoders(self,x):\n","                    \n","                return self.Decoder(x)\n","                    \n","        def forward(self,x):\n","                encode=self.encoder(x)\n","                return self.decoders(encode)\n","\n","\n","#img _level D\n","class Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, act=True):\n","        super().__init__()\n","        self.act = act\n","        self.sn_conv = spectral_norm(nn.Conv2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size,\n","                stride, \n","                padding,\n","                padding_mode=\"zeros\" # Author's code used slim.convolution2d, which is using SAME padding (zero padding in pytorch) \n","            ))\n","        self.LReLU = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","    def forward(self, x):\n","        x = self.sn_conv(x)\n","        if self.act:\n","            x = self.LReLU(x)\n","        return x\n","class Discriminator_S(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=3, features=[32, 64, 128,256]):\n","        super().__init__()\n","        \n","        self.model = nn.Sequential(\n","           \n","            #k3n32s2\n","            Block(in_channels, features[0], kernel_size=3, stride=2, padding=1),#b,32,128,128\n","            #k3n32s1\n","            Block(features[0], features[0], kernel_size=3, stride=1, padding=1),#b,32,128,128\n","\n","            #k3n64s2\n","            Block(features[0], features[1], kernel_size=3, stride=2, padding=1),#b,64,64,64\n","            #k3n64s1\n","            Block(features[1], features[1], kernel_size=3, stride=1, padding=1),#b,64,64,64\n","\n","            #k3n128s2\n","            Block(features[1], features[2], kernel_size=3, stride=2, padding=1),#b,128,32,32\n","            #k3n128s1\n","            Block(features[2], features[2], kernel_size=3, stride=1, padding=1),#b,128,32,32\n","\n","            #k1n1s1\n","            Block(features[2], out_channels, kernel_size=1, stride=1, padding=0, act=False)#b,3,32,32\n","        )\n","        \n","    def forward(self, x):\n","        x = self.model(x)\n","        return x\n","    \n","    \n","# #patchgan  \n","# class PatchGAN_D(nn.Module):\n","#           def __init__(self,hw=64,out_channels=1,norm=nn.InstanceNorm2d,use_bias=False,use_sn=True):\n","#                   super(PatchGAN_D,self).__init__()\n","#                   model=[]\n","#                   self.use_sn=use_sn\n","#                   #平面卷积\n","#                   model+=[nn.Conv2d(out_channels,hw,kernel_size=3,stride=1,padding=1,bias=True),\n","#                           nn.LeakyReLU(0.2,True)\n","#                   ]\n","#                   #下采样\n","#                   model+=[nn.Conv2d(hw,hw*2,kernel_size=3,stride=2,padding=1,bias=True),\n","#                               nn.LeakyReLU(0.2,True),nn.Conv2d(hw*2,hw*4,kernel_size=3,stride=1,padding=1,bias=True),\n","#                               norm(hw*4),nn.LeakyReLU(0.2,True),\n","#                               nn.Conv2d(hw*4,hw*4,kernel_size=3,stride=2,padding=1,bias=True),\n","#                               nn.LeakyReLU(0.2,True),nn.Conv2d(hw*4,hw*8,kernel_size=3,stride=1,padding=1,bias=True),\n","#                               norm(hw*8),nn.LeakyReLU(0.2,True),\n","#                               nn.Conv2d(hw*8,1,kernel_size=3,stride=1,padding=1),nn.Sigmoid()\n","#                               ]\n","#                   if self.use_sn:\n","#                         for i in range(len(model)):\n","#                             if isinstance(model[i], nn.Conv2d):\n","#                                 model[i] = spectral_norm(model[i]) \n","#                   self.model=nn.Sequential(*model)\n","#           def forward(self,x):\n","#                     return self.model(x)\n","#判别器结构patchGAN\n","# class Discriminator(nn.Module):\n","#           def __init__(self):\n","#                   super(Discriminator_T,self).__init__()\n","#                   self.model=[PatchGAN_D()]\n","#                   self.model=nn.Sequential(*self.model)\n","                 \n","#           def forward(self,x):\n","#                 return self.model(x)\n","#           def init_train(self,grad=None):\n","#               for param in self.model.parameters():\n","#                   param.requires_grad = grad \n","\n","#VGG各种权重下载地址\n","model_weight_urls = {\n","    \"vgg11\": \"https://download.pytorch.org/models/vgg11-8a719046.pth\",\n","    \"vgg13\": \"https://download.pytorch.org/models/vgg13-19584684.pth\",\n","    \"vgg16\": \"https://download.pytorch.org/models/vgg16-397923af.pth\",\n","    \"vgg19\": \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\",\n","    \"vgg11_bn\": \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\",\n","    \"vgg13_bn\": \"https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\",\n","    \"vgg16_bn\": \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\",\n","    \"vgg19_bn\": \"https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\",\n","}\n","\n","#VGG19\n","class VGG19(nn.Module):\n","    def __init__(self,batch_norm=False,num_classes=1000):\n","        super(VGG19, self).__init__()\n","        self.cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512,\n","                    'M']\n","        self.batch_norm=batch_norm\n","        self.num_clases = num_classes\n","        self.features=self.make_layers(self.cfg,self.batch_norm)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 7 * 7, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","    def make_layers(self, cfg, batch_norm=False):\n","        layers = []\n","        in_channels = 3\n","        for v in cfg:\n","            if v == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","                if batch_norm:\n","                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n","                else:\n","                    layers += [conv2d, nn.ReLU(inplace=True)]\n","                in_channels = v\n","        return nn.Sequential(*layers)\n","    def forward(self,x):\n","        module_list = list(self.features.modules())\n","        for l in module_list[1:27]:  # conv4_4\n","            x = l(x)\n","        return x\n","#SL-LIN\n","class content_struct(nn.Module):\n","    def __init__(self):\n","        super(content_struct, self).__init__()\n","#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","#         self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n","#         self.fc1 = nn.Linear(128 * 16 * 16, 512)\n","#         self.fc2 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","#         x = nn.functional.relu(self.conv1(x))\n","#         x = nn.functional.relu(self.conv2(x))\n","        x = self.pool(x)\n","#         x = x.view(-1, 128 * 16 * 16)\n","#         x = nn.functional.relu(self.fc1(x))\n","#         x = self.fc2(x)\n","        return x\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-29T04:14:07.603063Z","iopub.status.busy":"2024-03-29T04:14:07.602734Z","iopub.status.idle":"2024-03-29T04:14:07.703273Z","shell.execute_reply":"2024-03-29T04:14:07.702305Z","shell.execute_reply.started":"2024-03-29T04:14:07.603036Z"},"tags":[],"trusted":true},"outputs":[],"source":["from torch import nn\n","import torch\n","import time as t\n","from  tqdm import tqdm\n","from torch import optim\n","from torchvision import transforms\n","# from torchvision.transforms import InterpolationMode\n","from torchvision.utils import save_image\n","from torch.utils import data\n","from torch.nn.functional import interpolate\n","class MyGAN(object):\n","    def __init__(self, args):\n","        super().__init__()\n","        # 定义配置\n","        self.device = args.device\n","        self.result_dir = args.result_dir\n","        self.checkpoint_dir = args.checkpoint_dir\n","        self.dataset = args.dataset\n","        self.data_dir = args.data_dir\n","        self.test_dir = args.test_dir\n","        self.isTrain = args.isTrain\n","        self.isTest = args.isTest\n","        self.train_init = args.train_init\n","        self.epoch = args.epoch\n","        self.pre_epoch = args.pre_epoch\n","        self.cpu_count = args.cpu_count\n","        # 定义模型参数\n","        self.input_c = args.input_c\n","        self.hw = args.hw\n","        self.b1 = args.b1\n","        self.b2 = args.b2\n","        self.y1 = args.y1\n","        self.y2 = args.y2\n","        self.g_lr = args.g_lr\n","        self.d_lr = args.d_lr\n","        self.latent_dim = args.latent_dim\n","        self.pk = args.patch_size\n","        self.s = args.s\n","        self.decay_lr = args.decay_lr\n","        self.init_lr = args.init_lr\n","        self.batch_size = args.batch_size\n","        self.save_pred = args.save_pred\n","        # 模型权重参数w\n","        self.weight_content = args.weight_content\n","        self.weight_surface = args.weight_surface\n","        self.weight_testure = args.weight_testure\n","        self.weight_struct = args.weight_struct\n","\n","        self.weight_style = args.weight_style\n","        self.weight_decay = args.weight_decay\n","        self.tv_weight = args.weight_tv\n","        # 定义模型\n","        self.G = Generator().to(self.device)\n","        self.D = Discriminator_S().to(self.device)\n","        self.D_patch = Discriminator_T().to(self.device)\n","        self.style_net=StyleEncoder().to(self.device)\n","        self.sct=utm().to(self.device)\n","        self.vgg19 = VGG19().to(self.device)\n","        self.vgg19.eval()\n","        self.p=content_struct().to(self.device)\n","        # 模型初始化\n","        self.G.apply(init_weights)\n","        self.D.apply(init_weights)\n","        self.D_patch.apply(init_weights)\n","        self.sct.apply(init_weights)\n","        self.style_net.apply(init_weights)\n","        self.vgg19.load_state_dict(torch.load('/kaggle/input/vgg19p/vgg19.pth'))\n","        # 定义优化器\n","        if self.train_init:\n","             self.optim_G = optim.Adam(self.G.parameters(), lr=self.init_lr, betas=(self.b1, self.b2))\n","        else:        \n","             self.optim_G = optim.Adam(self.G.parameters(), lr=self.g_lr, betas=(self.b1, self.b2))\n","        self.op_style_net=optim.Adam(self.style_net.parameters(), lr=self.d_lr, betas=(self.b1, self.b2))\n","        self.optim_sct = optim.Adam(self.sct.parameters(), lr=self.d_lr, betas=(self.b1, self.b2))\n","        self.optim_D = optim.Adam(self.D.parameters(), lr=self.d_lr, betas=(self.b1, self.b2))\n","        self.optim_D_Patch = optim.Adam(self.D_patch.parameters(), lr=self.g_lr, betas=(self.b1, self.b2))\n","        # 定义损失\n","        self.l1_loss = nn.L1Loss()\n","        self.huber = nn.SmoothL1Loss()\n","        self.gan_loss = nn.MSELoss()\n","        self.tv_loss = VariationLoss(1)\n","        self.lsty=nn.CrossEntropyLoss()\n","        self._rgb_to_yuv_kernel = torch.tensor([\n","            [0.299, -0.14714119, 0.61497538],\n","            [0.587, -0.28886916, -0.51496512],\n","            [0.114, 0.43601035, -0.10001026]\n","        ]).float().to(self.device)\n","        # 辅助器).to(self.device)\n","        self.gf = GuidedFilter()\n","        self.query = Queue()\n","        self.edge_exactor = CannyFilter().to(self.device)\n","        # 打印配置\n","        print(\"##### Information #####\")\n","        print(\"# device : \", self.device)\n","        print(f\"线程:{int(self.cpu_count)}\")\n","        print(\"# dataset : \", self.dataset)\n","        print(f'# G_num :{print_network(self.G)}')\n","        print(f'# D_num :{print_network(self.D)}')\n","        print(f'# D_Patch_num :{print_network(self.D_patch)}')\n","        print(\"# batch_size : \", self.batch_size)\n","        print(\"# epoch : \", self.epoch)\n","        print(\"# pre epoch : \", self.pre_epoch)\n","        print(\"# init_train : \", self.train_init)\n","        print(\"# training image size [H, W] : \", self.hw)\n","        print(\"# content,style,suface,testure,color_weight,tv_weight: \", self.weight_content, self.weight_style,self.weight_surface,\n","              self.weight_testure,   self.weight_struct,self.tv_weight)\n","        print(\"# init_lr,g_lr,d_lr: \", self.init_lr, self.g_lr, self.d_lr)\n","    #生成假图.to(self.device)  \n","\n","    # 读取数据集\n","    def load_data(self, epoch_test=False,high=False):\n","        self.mode = epoch_test\n","        self.high=high\n","        # 增强操作\n","        train_trans = [transforms.Resize(286),\n","               transforms.CenterCrop(256),\n","               transforms.RandomHorizontalFlip(0.5),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n","        test_trans = [transforms.Resize([512,512]),\n","                      transforms.ToTensor(),\n","                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n","        if self.mode and self.isTrain:\n","            data_loader = data.DataLoader(ImagePools(root=self.data_dir, trans=test_trans,mode=self.mode),\n","                                          batch_size=4,\n","                                          pin_memory=True,\n","                                          drop_last=True\n","                                          , num_workers=0)\n","        if self.isTrain and not self.mode:\n","            data_loader = data.DataLoader(ImagePools(root=self.data_dir, trans=train_trans),\n","                                          batch_size=self.batch_size, pin_memory=False,\n","                                          drop_last=True\n","                                          , num_workers=0)\n","\n","        if self.isTest and self.mode:\n","            if self.high:\n","                 data_loader = data.DataLoader(ImagePools(root=self.data_dir, trans=test_trans,mode=self.mode,high=self.high), batch_size=1, num_workers=2)\n","            else:\n","                data_loader = data.DataLoader(ImagePools(root=self.data_dir, trans=test_trans,mode=self.mode), batch_size=1, num_workers=2)\n","        if self.isTest and not self.mode:\n","            data_loader = data.DataLoader(ImagePools(root=self.data_dir, trans=test_trans), batch_size=1, num_workers=2)    \n","        return data_loader\n","\n","\n","    # 加载灰度patch\n","    def load_patch(self, real, fake):\n","        # 进行灰度颜色改变\n","        real_gry= color_shift(real)\n","        fake_gry=color_shift(fake)\n","        return real_gry, fake_gry\n","\n","    # conten loss\n","    def content_loss(self, fake, real):\n","        _,c,w,h=fake.shape\n","        out_con = self.l1_loss(fake,real)\n","        return out_con\n","\n","    # dis loss\n","    def discriminator_loss(self, real, fake):\n","        real_loss = torch.mean(torch.square(real - 1.0))\n","        fake_loss = torch.mean(torch.square(fake))\n","        loss = real_loss + fake_loss\n","        return loss\n","\n","    def generator_loss(self, fake):\n","        fake_loss = torch.mean(torch.square(fake - 1.0))\n","        return fake_loss\n","    # dis loss\n","    def discriminator_gram_loss(self, real,fake):\n","        real = gram(real)\n","        fake = gram(fake)\n","        real_loss = torch.mean(torch.square(real - 1.0))\n","        fake_loss = torch.mean(torch.square(fake))\n","        loss = real_loss + fake_loss\n","        return loss\n","\n","    def generator_gram_loss(self, fake):\n","        fake = gram(fake)\n","        fake_loss = torch.mean(torch.square(fake - 1.0))\n","        return fake_loss\n","\n","    # 训练\n","    def train(self):\n","        data_loader = self.load_data()\n","        count = len(data_loader)\n","       \n","#             self.classifier_optim.param_groups[0]['lr']-=self.lr/(self.iter//2)\n","        start_t = t.time()\n","        self.G.train()\n","        if self.train_init:\n","            print(\"=============================pre train phase==============================\")\n","            for epoch in tqdm(range(self.pre_epoch)):\n","                for i, (x, y) in enumerate(data_loader):\n","                    x, y = x.to(self.device), y.to(self.device)\n","                    # 预训练阶段\n","                    requires_grad(self.D, False)\n","                    requires_grad(self.D_patch, False)\n","                    requires_grad(self.style_net, False)\n","                    self.optim_G.zero_grad()\n","                    self.optim_sct.zero_grad()\n","                    con_code= self.G.encoder(x)\n","                    con_code=self.sct(con_code,init=True)\n","                    fake1_img=self.G.decoders(con_code)\n","                    real_con = self.vgg19(x)\n","                    fake_con1 = self.vgg19(fake1_img)\n","                    con_loss_1=self.content_loss(fake_con1,real_con.detach())\n","                    #128\n","                    real_con2=interpolate(x,scale_factor=0.5,mode='bilinear')\n","                    real_con2=interpolate(real_con2,scale_factor=2,mode='bilinear')\n","                    fake_con2=interpolate(fake1_img,scale_factor=0.5,mode='bilinear')\n","                    fake_con2=interpolate(fake_con2, scale_factor=2, mode='bilinear')\n","                    con_loss_2=self.content_loss(fake_con2,real_con2.detach())\n","                    #64    \n","                    real_con3=interpolate(x,scale_factor=0.25,mode='bilinear')\n","                    real_con3=interpolate(real_con3,scale_factor=4,mode='bilinear')\n","                    fake_con3=interpolate(fake1_img,scale_factor=0.25,mode='bilinear')\n","                    fake_con3=interpolate(fake_con3, scale_factor=4, mode='bilinear')\n","                    con_loss_3= self.content_loss(fake_con3,real_con3.detach())\n","                    con_loss=(con_loss_1+con_loss_2+con_loss_3)/3*self.weight_content\n","\n","                    con_loss.backward()\n","                    self.optim_G.step()\n","                    self.optim_sct.step()\n","                    end_epoch_t = t.time()\n","                    print(\n","                        f\"epoch:[{epoch + 1}/{self.pre_epoch}],iter:[{i + 1}/{count}],loss_G:{con_loss},G_lr:{self.optim_G.param_groups[0]['lr']},\"\n","                        f\"time:{time_change(end_epoch_t - start_t)}\")\n","                if epoch % self.save_pred == 0:\n","                    self.save_img1(epoch)\n","        else:\n","            print('==========================start train=====================================')\n","            for epoch in tqdm(range(self.epoch)):\n","                 # 学习率衰退\n","                if epoch>99:\n","                    self.optim_G.param_groups[0]['lr']-=0.0002/50\n","                    self.optim_D.param_groups[0]['lr']-=0.0002/50\n","                    self.optim_D_Patch.param_groups[0]['lr']-=0.0002/50\n","                    self.optim_sct.param_groups[0]['lr']-=0.0002/50\n","                    self.op_style_net.param_groups[0]['lr']-=0.0002/50\n","                for i, (x, y) in enumerate(data_loader):\n","                    x, y = x.to(self.device), y.to(self.device)\n","                    self.D.train(), self.D_patch.train()\n","                    # zero grident\n","                    self.optim_D.zero_grad()\n","                    self.optim_D_Patch.zero_grad()\n","                    self.op_style_net.zero_grad()\n","                    self.optim_sct.zero_grad()\n","                    # D\n","                    style_code=self.style_net(y)\n","                    content_code= self.G.encoder(x)\n","                    share_code=self.sct(content_code,style_code)\n","                    fake_img=self.G.decoders(share_code)\n","              # 减少模型震荡\n","                    # surface\n","                    gf_real_img_h =interpolate(y,scale_factor=2,mode='bilinear')\n","                    gf_fake_img_h =interpolate(fake_img,scale_factor=2,mode='bilinear')\n","                    gf_real_img_h = self.gf.guided_filter(gf_real_img_h, gf_real_img_h, r=5, eps=2e-1)\n","                    gf_fake_img_h = self.gf.guided_filter(gf_fake_img_h, gf_fake_img_h, r=5, eps=2e-1)\n","                    \n","                    d_real_logit=self.D(gf_real_img_h)\n","                    d_fake_logit=self.D(gf_fake_img_h.detach())\n","                    d_surface_loss= self.discriminator_loss(d_real_logit, d_fake_logit)\n","                    # testure\n","                    anime_gry_patch, fake_gry_patch= self.load_patch(gf_real_img_h, gf_fake_img_h)\n","#                     anime_gry_patch = interpolate(anime_gry_patch,scale_factor=2,mode='bilinear')\n","#                     fake_gry_patch = interpolate(fake_gry_patch,scale_factor=2,mode='bilinear')\n","                    real_patch_logit=self.D_patch(anime_gry_patch)\n","                    fake_patch_logit= self.D_patch(fake_gry_patch.detach())\n","                    d_testure_loss = self.discriminator_gram_loss(real_patch_logit,fake_patch_logit)\n","                    d_loss = (d_surface_loss + d_testure_loss) / 2\n","                    d_loss.backward()\n","                    self.optim_D.step()\n","                    self.optim_D_Patch.step()\n","                    # G\n","                    self.style_net.train()\n","                    self.sct.train()\n","                    self.optim_G.zero_grad()\n","                    self.op_style_net.zero_grad()\n","                    self.optim_sct.zero_grad()\n","                     #style  loss\n","                    style_code=self.style_net(y)\n","                    content_code= self.G.encoder(x)\n","                    share_code=self.sct(content_code,style_code)\n","                    fake1_img=self.G.decoders(share_code)\n","                    # surface\n","                    gf_fake_img_h1 =interpolate(fake1_img,scale_factor=2,mode='bilinear')\n","#                     y =interpolate(y,scale_factor=2,mode='bilinear')\n","                    gf_fake_img_h1 = self.gf.guided_filter(gf_fake_img_h1, gf_fake_img_h1, r=5, eps=2e-1)\n","                    \n","                    g_fake_logit1 = self.D(gf_fake_img_h1)\n","                    g_surface_loss =self.generator_loss(g_fake_logit1)*self.weight_surface\n","                    # testure\n","                    _, fake_gry_patch1 = self.load_patch(gf_fake_img_h1, gf_fake_img_h1)\n","#                     fake_gry_patch1 = interpolate(fake_gry_patch1,scale_factor=2,mode='bilinear')\n","                    fake_patch_logit=self.D_patch(fake_gry_patch1)\n","            \n","                    g_testure_loss = self.generator_gram_loss(fake_patch_logit)*self.weight_testure\n","                    # multi-level content loss\n","                    real_con = self.vgg19(x)\n","                    fake_con1 = self.vgg19(fake1_img)\n","                    con_loss_1=self.content_loss(fake_con1,real_con.detach())\n","                    #128\n","                    real_con2=interpolate(x,scale_factor=0.5,mode='bilinear')\n","                    real_con2=interpolate(real_con2,scale_factor=2,mode='bilinear')\n","                    fake_con2=interpolate(fake1_img,scale_factor=0.5,mode='bilinear')\n","                    fake_con2=interpolate(fake_con2, scale_factor=2, mode='bilinear')\n","                    con_loss_2=self.content_loss(fake_con2,real_con2.detach())\n","                    #64    \n","                    real_con3=interpolate(x,scale_factor=0.25,mode='bilinear')\n","                    real_con3=interpolate(real_con3,scale_factor=4,mode='bilinear')\n","                    fake_con3=interpolate(fake1_img,scale_factor=0.25,mode='bilinear')\n","                    fake_con3=interpolate(fake_con3, scale_factor=4, mode='bilinear')\n","                    con_loss_3= self.content_loss(fake_con3,real_con3.detach())\n","                    con_loss=(con_loss_1+con_loss_2+con_loss_3)/3*self.weight_content\n","                    # tv loss\n","                    tv_loss= self.tv_loss(fake1_img)\n","                     # color re LOSS\n","                    col_real_img = rgb_to_yuv(x, self._rgb_to_yuv_kernel)\n","                    col_fake_img = rgb_to_yuv(fake1_img, self._rgb_to_yuv_kernel)\n","                    col_loss = 10* (\n","                            self.l1_loss(col_real_img[:, 0, :, :], col_fake_img[:, 0, :, :]) + self.huber(\n","                            col_real_img[:, 1, :, :], col_fake_img[:, 1, :, :]) + \\\n","                                self.huber(col_real_img[:, 2, :, :], col_fake_img[:, 2, :, :]))\n","\n","                    g_loss = (g_surface_loss + g_testure_loss +con_loss + tv_loss +col_loss)/5\n","                    #                     else:\n","                    g_loss.backward()\n","                    self.optim_G.step()\n","                    self.op_style_net.step()\n","                    self.optim_sct.step()\n","                    end_epoch_t = t.time()\n","                    print(f\"epoch:[{epoch + 1}/{self.epoch}],iter:[{i + 1}/{count}],loss_G:{g_loss},loss_d:{d_loss},G_lr:{self.optim_G.param_groups[0]['lr']},D_lr:{self.optim_D.param_groups[0]['lr']},time:{time_change(end_epoch_t - start_t)}\")\n","                if (epoch + 1) % self.save_pred == 0:\n","\n","                    with torch.no_grad():\n","                        self.save_model()\n","#                          self.save_img(epoch)\n","    def save_img1(self, epoch):\n","        test_sample_num =5\n","        self.G.eval()\n","        data_loader = self.load_data()\n","        for j in range(test_sample_num):\n","            for i, (x, y) in tqdm(enumerate(data_loader)):\n","                break\n","            x = x.to(self.device)\n","            y = y.to(self.device)\n","            content_code= self.G.encoder(x)\n","            fake_img1=self.G.decoders(content_code) # 生成假图\n","           \n","            image = torch.cat((x*0.5+0.5, fake_img1*0.5+0.5), axis=3)\n","            save_image(image, os.path.join(self.result_dir, self.dataset, 'img', f\"train_{j}{epoch}.png\"))\n","        print(\"训练集测试图像生成成功！\")\n","        self.save_model()\n","        self.G.train(),self.style_net.train(),self.sct.train()\n","\n","    def save_img(self, epoch):\n","        test_sample_num =1\n","        self.G.eval(),self.style_net.eval(),self.sct.eval()\n","        data_loader = self.load_data()\n","        for j in range(test_sample_num):\n","            for i, (x, y) in tqdm(enumerate(data_loader)):\n","                break\n","            x = x.to(self.device)\n","            y = y.to(self.device)\n","            rand_style = torch.randn([self.batch_size,self.hw,self.latent_dim,self.latent_dim]).to(self.device).requires_grad_()\n","            style_code=self.style_net(y)\n","            content_code= self.G.encoder(x)\n","            share_code=self.sct(content_code,style_code,rand_style)\n","            fake_img1=self.G.decoders(share_code) # 生成假图\n","            fake_img2 =self.gf.guided_filter(x, fake_img1, r=1)\n","            image = torch.cat((x*0.5+0.5, fake_img1*0.5+0.5,fake_img2*0.5+0.5), axis=3)\n","            save_image(image, os.path.join(self.result_dir, self.dataset, 'img', f\"train_{j}{epoch}.png\"))\n","        print(\"训练集测试图像生成成功！\")\n","        test_loader = self.load_data(epoch_test=True)\n","        for i, (x, y) in tqdm(enumerate(test_loader)):\n","            break\n","        x = x.to(self.device)\n","        y = y.to(self.device)\n","#         style1_code,_ = self.style_net(lr_test)\n","        rand_style = torch.randn([4,self.hw,self.latent_dim,self.latent_dim]).to(self.device).requires_grad_()\n","        style_code=self.style_net(y)\n","        content_code= self.G.encoder(x)\n","        share_code=self.sct(content_code,style_code,rand_style)\n","        fake_img=self.G.decoders(share_code)\n","        lr_img2 =self.gf.guided_filter(x, fake_img, r=1)\n","        image = torch.cat((x*0.5+0.5, fake_img*0.5+0.5,lr_img2*0.5+0.5), axis=3)\n","        save_image(image, os.path.join(self.result_dir, self.dataset, 'img', f\"test_{epoch}.png\"))\n","        print(\"高分辨率测试集测试图像生成成功！\")\n","        self.save_model()\n","        self.G.train(),self.style_net.train(),self.sct.train()\n","\n","    # 保存模型\n","    def save_model(self):\n","        params = {}\n","        params[\"G\"] = self.G.state_dict()\n","        params['sct']=self.sct.state_dict()\n","        params[\"style\"] = self.style_net.state_dict()\n","        params[\"D\"] = self.D.state_dict()\n","        params[\"D_patch\"] = self.D_patch.state_dict()\n","        torch.save(params, os.path.join(self.result_dir, self.dataset, self.checkpoint_dir,\n","                                        f'checkpoint_{self.dataset}.pth'))\n","        print(\"保存模型成功！\")\n","\n","    # 加载模型\n","    def load_model(self):\n","        params = torch.load(self.test_dir)\n","        self.G.load_state_dict(params['G'])\n","        self.sct.load_state_dict(params['sct'])\n","        self.style_net.load_state_dict(params['style'])\n","        self.D.load_state_dict(params['D'])\n","        self.D_patch.load_state_dict(params['D_patch'])\n","        print(\"加载模型成功！\")\n","\n","    def test(self):\n","        self.load_model()\n","        test_sample_num =1\n","        self.G.eval(),self.style_net.eval(),self.sct.eval()\n","        data_loader = self.load_data()\n","        for j in range(test_sample_num):\n","            for i, (x, y) in tqdm(enumerate(data_loader)):\n","                x = x.to(self.device)\n","                y = y.to(self.device)\n","                # rand_style = torch.randn([2,self.hw,self.latent_dim,self.latent_dim]).to(self.device).requires_grad_()\n","                style_code=self.style_net(y)\n","                content_code= self.G.encoder(x)\n","                share_code=self.sct(content_code,style_code)\n","                fake_img=self.G.decoders(share_code) # 生成假图\n","                save_image(fake_img[0]*0.5+0.5 , os.path.join(self.result_dir, self.dataset, 'img', f'anime_lr{j}to{i}.png'))\n","        print(\"训练集测试图像生成成功！\")\n","        # self.high_test()\n","    def  high_test(self):  \n","        self.load_model()\n","        data_loader = self.load_data(epoch_test=True,high=True)\n","        self.G.eval(),self.style_net.eval(),self.sct.eval()\n","        for i, (x, y) in tqdm(enumerate(data_loader)):\n","            x = x.to(self.device)\n","            y = y.to(self.device)\n","            # rand_style = torch.randn([1,self.hw,self.latent_dim,self.latent_dim]).to(self.device).requires_grad_()\n","            style_code=self.style_net(y)\n","            content_code= self.G.encoder(x)\n","            share_code=self.sct(content_code,style_code)\n","            fake_img=self.G.decoders(share_code)\n","            image = torch.cat((x*0.5+0.5, fake_img*0.5+0.5), axis=3)\n","            save_image(image, os.path.join(self.result_dir, self.dataset, 'img', f\"test_high{i}.png\"))\n","            print(\"高分辨率测试集测试图像生成成功！\")\n","           "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-03-29T04:14:07.704973Z","iopub.status.busy":"2024-03-29T04:14:07.704607Z"},"jupyter":{"outputs_hidden":true},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["##### Information #####\n","# device :  cuda\n","线程:10\n","# dataset :  hayao\n","# G_num :11600940\n","# D_num :287395\n","# D_Patch_num :389665\n","# batch_size :  4\n","# epoch :  30\n","# pre epoch :  20\n","# init_train :  False\n","# training image size [H, W] :  256\n","# content,style,suface,testure,color_weight,tv_weight:  2 2 5 5 2 1\n","# init_lr,g_lr,d_lr:  0.002 0.0002 0.0002\n","加载模型成功！\n","training on cuda\n","==========================start train=====================================\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/30 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["epoch:[1/30],iter:[1/1611],loss_G:1.740273118019104,loss_d:0.9257938861846924,G_lr:0.0002,D_lr:0.0002,time:00h00m01s\n","epoch:[1/30],iter:[2/1611],loss_G:1.7380377054214478,loss_d:3.6916682720184326,G_lr:0.0002,D_lr:0.0002,time:00h00m02s\n","epoch:[1/30],iter:[3/1611],loss_G:1.8028160333633423,loss_d:0.818515419960022,G_lr:0.0002,D_lr:0.0002,time:00h00m03s\n","epoch:[1/30],iter:[4/1611],loss_G:1.8454296588897705,loss_d:0.9271384477615356,G_lr:0.0002,D_lr:0.0002,time:00h00m03s\n","epoch:[1/30],iter:[5/1611],loss_G:1.7906092405319214,loss_d:0.8936306238174438,G_lr:0.0002,D_lr:0.0002,time:00h00m04s\n","epoch:[1/30],iter:[6/1611],loss_G:1.510707139968872,loss_d:0.8382182121276855,G_lr:0.0002,D_lr:0.0002,time:00h00m05s\n","epoch:[1/30],iter:[7/1611],loss_G:1.1499974727630615,loss_d:0.7436254024505615,G_lr:0.0002,D_lr:0.0002,time:00h00m06s\n","epoch:[1/30],iter:[8/1611],loss_G:0.8589025735855103,loss_d:0.7108334302902222,G_lr:0.0002,D_lr:0.0002,time:00h00m06s\n","epoch:[1/30],iter:[9/1611],loss_G:0.8870100975036621,loss_d:0.6727720499038696,G_lr:0.0002,D_lr:0.0002,time:00h00m07s\n","epoch:[1/30],iter:[10/1611],loss_G:1.1247223615646362,loss_d:0.6857545375823975,G_lr:0.0002,D_lr:0.0002,time:00h00m08s\n","epoch:[1/30],iter:[11/1611],loss_G:1.1934049129486084,loss_d:0.6685439348220825,G_lr:0.0002,D_lr:0.0002,time:00h00m08s\n","epoch:[1/30],iter:[12/1611],loss_G:0.9869375228881836,loss_d:0.7080233693122864,G_lr:0.0002,D_lr:0.0002,time:00h00m09s\n","epoch:[1/30],iter:[13/1611],loss_G:0.984025776386261,loss_d:0.7034614682197571,G_lr:0.0002,D_lr:0.0002,time:00h00m10s\n","epoch:[1/30],iter:[14/1611],loss_G:0.9792048335075378,loss_d:0.7095475196838379,G_lr:0.0002,D_lr:0.0002,time:00h00m10s\n","epoch:[1/30],iter:[15/1611],loss_G:0.9859533309936523,loss_d:0.6544709801673889,G_lr:0.0002,D_lr:0.0002,time:00h00m11s\n","epoch:[1/30],iter:[16/1611],loss_G:0.8606613278388977,loss_d:0.6551074981689453,G_lr:0.0002,D_lr:0.0002,time:00h00m12s\n","epoch:[1/30],iter:[17/1611],loss_G:1.099234938621521,loss_d:0.6762101054191589,G_lr:0.0002,D_lr:0.0002,time:00h00m12s\n","epoch:[1/30],iter:[18/1611],loss_G:1.125351905822754,loss_d:0.6364805102348328,G_lr:0.0002,D_lr:0.0002,time:00h00m13s\n","epoch:[1/30],iter:[19/1611],loss_G:1.0705912113189697,loss_d:0.6871000528335571,G_lr:0.0002,D_lr:0.0002,time:00h00m14s\n","epoch:[1/30],iter:[20/1611],loss_G:0.9412440657615662,loss_d:0.6983547210693359,G_lr:0.0002,D_lr:0.0002,time:00h00m14s\n","epoch:[1/30],iter:[21/1611],loss_G:0.8685705065727234,loss_d:0.6551901698112488,G_lr:0.0002,D_lr:0.0002,time:00h00m15s\n","epoch:[1/30],iter:[22/1611],loss_G:0.9572997093200684,loss_d:0.681416392326355,G_lr:0.0002,D_lr:0.0002,time:00h00m16s\n","epoch:[1/30],iter:[23/1611],loss_G:1.0128401517868042,loss_d:0.6606645584106445,G_lr:0.0002,D_lr:0.0002,time:00h00m17s\n","epoch:[1/30],iter:[24/1611],loss_G:1.0049567222595215,loss_d:0.6416351795196533,G_lr:0.0002,D_lr:0.0002,time:00h00m17s\n","epoch:[1/30],iter:[25/1611],loss_G:1.0017445087432861,loss_d:0.6445168256759644,G_lr:0.0002,D_lr:0.0002,time:00h00m18s\n","epoch:[1/30],iter:[26/1611],loss_G:1.0839893817901611,loss_d:0.6944795846939087,G_lr:0.0002,D_lr:0.0002,time:00h00m19s\n","epoch:[1/30],iter:[27/1611],loss_G:0.9066750407218933,loss_d:0.6272144913673401,G_lr:0.0002,D_lr:0.0002,time:00h00m19s\n","epoch:[1/30],iter:[28/1611],loss_G:0.9519548416137695,loss_d:0.6549798250198364,G_lr:0.0002,D_lr:0.0002,time:00h00m20s\n","epoch:[1/30],iter:[29/1611],loss_G:0.9904985427856445,loss_d:0.6460334062576294,G_lr:0.0002,D_lr:0.0002,time:00h00m21s\n","epoch:[1/30],iter:[30/1611],loss_G:0.903795063495636,loss_d:0.624719500541687,G_lr:0.0002,D_lr:0.0002,time:00h00m21s\n","epoch:[1/30],iter:[31/1611],loss_G:0.9986954927444458,loss_d:0.6533199548721313,G_lr:0.0002,D_lr:0.0002,time:00h00m22s\n","epoch:[1/30],iter:[32/1611],loss_G:0.9861600995063782,loss_d:0.6178528070449829,G_lr:0.0002,D_lr:0.0002,time:00h00m23s\n","epoch:[1/30],iter:[33/1611],loss_G:1.0380103588104248,loss_d:0.6258975267410278,G_lr:0.0002,D_lr:0.0002,time:00h00m23s\n","epoch:[1/30],iter:[34/1611],loss_G:0.9752273559570312,loss_d:0.6226690411567688,G_lr:0.0002,D_lr:0.0002,time:00h00m24s\n","epoch:[1/30],iter:[35/1611],loss_G:1.0882658958435059,loss_d:0.5836567878723145,G_lr:0.0002,D_lr:0.0002,time:00h00m25s\n","epoch:[1/30],iter:[36/1611],loss_G:1.1818302869796753,loss_d:0.6252747178077698,G_lr:0.0002,D_lr:0.0002,time:00h00m25s\n","epoch:[1/30],iter:[37/1611],loss_G:1.0755484104156494,loss_d:0.644141674041748,G_lr:0.0002,D_lr:0.0002,time:00h00m26s\n","epoch:[1/30],iter:[38/1611],loss_G:0.9694642424583435,loss_d:0.763568639755249,G_lr:0.0002,D_lr:0.0002,time:00h00m27s\n","epoch:[1/30],iter:[39/1611],loss_G:1.172014594078064,loss_d:0.6312940120697021,G_lr:0.0002,D_lr:0.0002,time:00h00m28s\n","epoch:[1/30],iter:[40/1611],loss_G:1.0424137115478516,loss_d:0.6178079843521118,G_lr:0.0002,D_lr:0.0002,time:00h00m28s\n","epoch:[1/30],iter:[41/1611],loss_G:0.9219374656677246,loss_d:0.7047213912010193,G_lr:0.0002,D_lr:0.0002,time:00h00m29s\n","epoch:[1/30],iter:[42/1611],loss_G:1.0839861631393433,loss_d:0.5855368375778198,G_lr:0.0002,D_lr:0.0002,time:00h00m30s\n","epoch:[1/30],iter:[43/1611],loss_G:1.0364948511123657,loss_d:0.6546648144721985,G_lr:0.0002,D_lr:0.0002,time:00h00m30s\n","epoch:[1/30],iter:[44/1611],loss_G:0.9458829164505005,loss_d:0.6010820865631104,G_lr:0.0002,D_lr:0.0002,time:00h00m31s\n","epoch:[1/30],iter:[45/1611],loss_G:0.9171543121337891,loss_d:0.6217821836471558,G_lr:0.0002,D_lr:0.0002,time:00h00m32s\n","epoch:[1/30],iter:[46/1611],loss_G:1.272733211517334,loss_d:0.6592495441436768,G_lr:0.0002,D_lr:0.0002,time:00h00m32s\n","epoch:[1/30],iter:[47/1611],loss_G:1.1754885911941528,loss_d:0.653123140335083,G_lr:0.0002,D_lr:0.0002,time:00h00m33s\n","epoch:[1/30],iter:[48/1611],loss_G:0.9642353057861328,loss_d:0.6864407062530518,G_lr:0.0002,D_lr:0.0002,time:00h00m34s\n","epoch:[1/30],iter:[49/1611],loss_G:0.8096410036087036,loss_d:0.6427447199821472,G_lr:0.0002,D_lr:0.0002,time:00h00m34s\n","epoch:[1/30],iter:[50/1611],loss_G:0.9474703669548035,loss_d:0.631939172744751,G_lr:0.0002,D_lr:0.0002,time:00h00m35s\n","epoch:[1/30],iter:[51/1611],loss_G:0.9376307725906372,loss_d:0.5858954191207886,G_lr:0.0002,D_lr:0.0002,time:00h00m36s\n","epoch:[1/30],iter:[52/1611],loss_G:0.8935673832893372,loss_d:0.6403481960296631,G_lr:0.0002,D_lr:0.0002,time:00h00m36s\n","epoch:[1/30],iter:[53/1611],loss_G:0.9186257719993591,loss_d:0.6379365921020508,G_lr:0.0002,D_lr:0.0002,time:00h00m37s\n","epoch:[1/30],iter:[54/1611],loss_G:0.9227785468101501,loss_d:0.5904194712638855,G_lr:0.0002,D_lr:0.0002,time:00h00m38s\n","epoch:[1/30],iter:[55/1611],loss_G:0.9548007845878601,loss_d:0.5709023475646973,G_lr:0.0002,D_lr:0.0002,time:00h00m38s\n","epoch:[1/30],iter:[56/1611],loss_G:1.0188734531402588,loss_d:0.6626647114753723,G_lr:0.0002,D_lr:0.0002,time:00h00m39s\n","epoch:[1/30],iter:[57/1611],loss_G:1.0161927938461304,loss_d:0.5993434190750122,G_lr:0.0002,D_lr:0.0002,time:00h00m40s\n","epoch:[1/30],iter:[58/1611],loss_G:0.8820497393608093,loss_d:0.6204432249069214,G_lr:0.0002,D_lr:0.0002,time:00h00m41s\n","epoch:[1/30],iter:[59/1611],loss_G:1.1445376873016357,loss_d:0.6443145871162415,G_lr:0.0002,D_lr:0.0002,time:00h00m41s\n","epoch:[1/30],iter:[60/1611],loss_G:1.1543655395507812,loss_d:0.6117483973503113,G_lr:0.0002,D_lr:0.0002,time:00h00m42s\n","epoch:[1/30],iter:[61/1611],loss_G:0.7610217332839966,loss_d:0.623598039150238,G_lr:0.0002,D_lr:0.0002,time:00h00m43s\n","epoch:[1/30],iter:[62/1611],loss_G:0.9738671183586121,loss_d:0.5529577732086182,G_lr:0.0002,D_lr:0.0002,time:00h00m43s\n","epoch:[1/30],iter:[63/1611],loss_G:1.0116679668426514,loss_d:0.5997333526611328,G_lr:0.0002,D_lr:0.0002,time:00h00m44s\n","epoch:[1/30],iter:[64/1611],loss_G:0.9598702788352966,loss_d:0.67710280418396,G_lr:0.0002,D_lr:0.0002,time:00h00m45s\n","epoch:[1/30],iter:[65/1611],loss_G:0.8963543772697449,loss_d:0.6464619636535645,G_lr:0.0002,D_lr:0.0002,time:00h00m45s\n","epoch:[1/30],iter:[66/1611],loss_G:0.959555447101593,loss_d:0.6180322170257568,G_lr:0.0002,D_lr:0.0002,time:00h00m46s\n","epoch:[1/30],iter:[67/1611],loss_G:0.9084573984146118,loss_d:0.612006425857544,G_lr:0.0002,D_lr:0.0002,time:00h00m47s\n","epoch:[1/30],iter:[68/1611],loss_G:0.9546992182731628,loss_d:0.617140531539917,G_lr:0.0002,D_lr:0.0002,time:00h00m47s\n","epoch:[1/30],iter:[69/1611],loss_G:0.9285861253738403,loss_d:0.5695228576660156,G_lr:0.0002,D_lr:0.0002,time:00h00m48s\n","epoch:[1/30],iter:[70/1611],loss_G:0.8055470585823059,loss_d:0.6397272348403931,G_lr:0.0002,D_lr:0.0002,time:00h00m49s\n","epoch:[1/30],iter:[71/1611],loss_G:0.9124442934989929,loss_d:0.5464714765548706,G_lr:0.0002,D_lr:0.0002,time:00h00m49s\n","epoch:[1/30],iter:[72/1611],loss_G:0.9331628084182739,loss_d:0.5823376178741455,G_lr:0.0002,D_lr:0.0002,time:00h00m50s\n","epoch:[1/30],iter:[73/1611],loss_G:0.9359272122383118,loss_d:0.5682814121246338,G_lr:0.0002,D_lr:0.0002,time:00h00m51s\n","epoch:[1/30],iter:[74/1611],loss_G:0.8976303935050964,loss_d:0.5934056043624878,G_lr:0.0002,D_lr:0.0002,time:00h00m52s\n","epoch:[1/30],iter:[75/1611],loss_G:1.0807647705078125,loss_d:0.7346881628036499,G_lr:0.0002,D_lr:0.0002,time:00h00m52s\n","epoch:[1/30],iter:[76/1611],loss_G:1.1850885152816772,loss_d:0.6730347871780396,G_lr:0.0002,D_lr:0.0002,time:00h00m53s\n","epoch:[1/30],iter:[77/1611],loss_G:0.9107502102851868,loss_d:0.6199051141738892,G_lr:0.0002,D_lr:0.0002,time:00h00m54s\n","epoch:[1/30],iter:[78/1611],loss_G:0.7762081027030945,loss_d:0.5733922123908997,G_lr:0.0002,D_lr:0.0002,time:00h00m54s\n","epoch:[1/30],iter:[79/1611],loss_G:0.9607596397399902,loss_d:0.6454935073852539,G_lr:0.0002,D_lr:0.0002,time:00h00m55s\n","epoch:[1/30],iter:[80/1611],loss_G:1.0027588605880737,loss_d:0.593602180480957,G_lr:0.0002,D_lr:0.0002,time:00h00m56s\n","epoch:[1/30],iter:[81/1611],loss_G:0.9245981574058533,loss_d:0.6274734735488892,G_lr:0.0002,D_lr:0.0002,time:00h00m56s\n","epoch:[1/30],iter:[82/1611],loss_G:0.9661738276481628,loss_d:0.5233016014099121,G_lr:0.0002,D_lr:0.0002,time:00h00m57s\n","epoch:[1/30],iter:[83/1611],loss_G:0.902340829372406,loss_d:0.5874438881874084,G_lr:0.0002,D_lr:0.0002,time:00h00m58s\n","epoch:[1/30],iter:[84/1611],loss_G:0.8936220407485962,loss_d:0.6013275384902954,G_lr:0.0002,D_lr:0.0002,time:00h00m58s\n","epoch:[1/30],iter:[85/1611],loss_G:1.076871395111084,loss_d:0.6005458831787109,G_lr:0.0002,D_lr:0.0002,time:00h00m59s\n","epoch:[1/30],iter:[86/1611],loss_G:0.9504755139350891,loss_d:0.5961881875991821,G_lr:0.0002,D_lr:0.0002,time:00h01m00s\n","epoch:[1/30],iter:[87/1611],loss_G:0.8452780842781067,loss_d:0.5513343811035156,G_lr:0.0002,D_lr:0.0002,time:00h01m00s\n","epoch:[1/30],iter:[88/1611],loss_G:0.9762890934944153,loss_d:0.6935569047927856,G_lr:0.0002,D_lr:0.0002,time:00h01m01s\n","epoch:[1/30],iter:[89/1611],loss_G:0.8745121955871582,loss_d:0.5996116399765015,G_lr:0.0002,D_lr:0.0002,time:00h01m02s\n","epoch:[1/30],iter:[90/1611],loss_G:1.0243321657180786,loss_d:0.5562408566474915,G_lr:0.0002,D_lr:0.0002,time:00h01m03s\n","epoch:[1/30],iter:[91/1611],loss_G:1.0294640064239502,loss_d:0.6147340536117554,G_lr:0.0002,D_lr:0.0002,time:00h01m03s\n","epoch:[1/30],iter:[92/1611],loss_G:0.9832807779312134,loss_d:0.5807923078536987,G_lr:0.0002,D_lr:0.0002,time:00h01m04s\n","epoch:[1/30],iter:[93/1611],loss_G:0.8474251627922058,loss_d:0.6400234699249268,G_lr:0.0002,D_lr:0.0002,time:00h01m05s\n","epoch:[1/30],iter:[94/1611],loss_G:1.010900855064392,loss_d:0.6178550720214844,G_lr:0.0002,D_lr:0.0002,time:00h01m05s\n","epoch:[1/30],iter:[95/1611],loss_G:0.981663703918457,loss_d:0.5806484222412109,G_lr:0.0002,D_lr:0.0002,time:00h01m06s\n","epoch:[1/30],iter:[96/1611],loss_G:1.054084300994873,loss_d:0.5901026129722595,G_lr:0.0002,D_lr:0.0002,time:00h01m07s\n","epoch:[1/30],iter:[97/1611],loss_G:1.0186318159103394,loss_d:0.6509212255477905,G_lr:0.0002,D_lr:0.0002,time:00h01m07s\n","epoch:[1/30],iter:[98/1611],loss_G:0.8460907936096191,loss_d:0.551814079284668,G_lr:0.0002,D_lr:0.0002,time:00h01m08s\n","epoch:[1/30],iter:[99/1611],loss_G:0.8995234370231628,loss_d:0.6469238996505737,G_lr:0.0002,D_lr:0.0002,time:00h01m09s\n","epoch:[1/30],iter:[100/1611],loss_G:0.9789041876792908,loss_d:0.5666373372077942,G_lr:0.0002,D_lr:0.0002,time:00h01m09s\n","epoch:[1/30],iter:[101/1611],loss_G:0.8996643424034119,loss_d:0.5599428415298462,G_lr:0.0002,D_lr:0.0002,time:00h01m10s\n","epoch:[1/30],iter:[102/1611],loss_G:0.9252514243125916,loss_d:0.5649010539054871,G_lr:0.0002,D_lr:0.0002,time:00h01m11s\n","epoch:[1/30],iter:[103/1611],loss_G:0.9000989198684692,loss_d:0.6129010915756226,G_lr:0.0002,D_lr:0.0002,time:00h01m11s\n","epoch:[1/30],iter:[104/1611],loss_G:1.0581179857254028,loss_d:0.5177750587463379,G_lr:0.0002,D_lr:0.0002,time:00h01m12s\n","epoch:[1/30],iter:[105/1611],loss_G:0.8937116861343384,loss_d:0.5490903854370117,G_lr:0.0002,D_lr:0.0002,time:00h01m13s\n","epoch:[1/30],iter:[106/1611],loss_G:0.8528705835342407,loss_d:0.6072484850883484,G_lr:0.0002,D_lr:0.0002,time:00h01m13s\n","epoch:[1/30],iter:[107/1611],loss_G:1.0902942419052124,loss_d:0.6311646699905396,G_lr:0.0002,D_lr:0.0002,time:00h01m14s\n","epoch:[1/30],iter:[108/1611],loss_G:1.0705994367599487,loss_d:0.6035360097885132,G_lr:0.0002,D_lr:0.0002,time:00h01m15s\n","epoch:[1/30],iter:[109/1611],loss_G:0.825229287147522,loss_d:0.6095914244651794,G_lr:0.0002,D_lr:0.0002,time:00h01m16s\n","epoch:[1/30],iter:[110/1611],loss_G:0.9660786986351013,loss_d:0.5690939426422119,G_lr:0.0002,D_lr:0.0002,time:00h01m16s\n","epoch:[1/30],iter:[111/1611],loss_G:0.8544689416885376,loss_d:0.6030711531639099,G_lr:0.0002,D_lr:0.0002,time:00h01m17s\n","epoch:[1/30],iter:[112/1611],loss_G:1.0007537603378296,loss_d:0.662624716758728,G_lr:0.0002,D_lr:0.0002,time:00h01m18s\n","epoch:[1/30],iter:[113/1611],loss_G:1.0056504011154175,loss_d:0.5522036552429199,G_lr:0.0002,D_lr:0.0002,time:00h01m18s\n","epoch:[1/30],iter:[114/1611],loss_G:0.8892537951469421,loss_d:0.6939548254013062,G_lr:0.0002,D_lr:0.0002,time:00h01m19s\n","epoch:[1/30],iter:[115/1611],loss_G:1.1068896055221558,loss_d:0.5432129502296448,G_lr:0.0002,D_lr:0.0002,time:00h01m20s\n","epoch:[1/30],iter:[116/1611],loss_G:0.8898088335990906,loss_d:0.6198627352714539,G_lr:0.0002,D_lr:0.0002,time:00h01m20s\n","epoch:[1/30],iter:[117/1611],loss_G:0.8601083755493164,loss_d:0.538077712059021,G_lr:0.0002,D_lr:0.0002,time:00h01m21s\n","epoch:[1/30],iter:[118/1611],loss_G:1.0219939947128296,loss_d:0.6032456159591675,G_lr:0.0002,D_lr:0.0002,time:00h01m22s\n","epoch:[1/30],iter:[119/1611],loss_G:0.9888703227043152,loss_d:0.5549108982086182,G_lr:0.0002,D_lr:0.0002,time:00h01m22s\n","epoch:[1/30],iter:[120/1611],loss_G:0.7747902274131775,loss_d:0.6077120304107666,G_lr:0.0002,D_lr:0.0002,time:00h01m23s\n","epoch:[1/30],iter:[121/1611],loss_G:0.889988362789154,loss_d:0.582428514957428,G_lr:0.0002,D_lr:0.0002,time:00h01m24s\n","epoch:[1/30],iter:[122/1611],loss_G:0.9563760757446289,loss_d:0.5651459693908691,G_lr:0.0002,D_lr:0.0002,time:00h01m24s\n","epoch:[1/30],iter:[123/1611],loss_G:0.9397045373916626,loss_d:0.6168432831764221,G_lr:0.0002,D_lr:0.0002,time:00h01m25s\n","epoch:[1/30],iter:[124/1611],loss_G:0.8382367491722107,loss_d:0.585849940776825,G_lr:0.0002,D_lr:0.0002,time:00h01m26s\n","epoch:[1/30],iter:[125/1611],loss_G:0.9620280265808105,loss_d:0.5794106721878052,G_lr:0.0002,D_lr:0.0002,time:00h01m27s\n","epoch:[1/30],iter:[126/1611],loss_G:0.9849972128868103,loss_d:0.6128481030464172,G_lr:0.0002,D_lr:0.0002,time:00h01m27s\n","epoch:[1/30],iter:[127/1611],loss_G:0.9412018060684204,loss_d:0.5771269798278809,G_lr:0.0002,D_lr:0.0002,time:00h01m28s\n","epoch:[1/30],iter:[128/1611],loss_G:0.9012784957885742,loss_d:0.5742537975311279,G_lr:0.0002,D_lr:0.0002,time:00h01m29s\n","epoch:[1/30],iter:[129/1611],loss_G:0.9184361696243286,loss_d:0.5611748099327087,G_lr:0.0002,D_lr:0.0002,time:00h01m29s\n","epoch:[1/30],iter:[130/1611],loss_G:0.8262635469436646,loss_d:0.5908365249633789,G_lr:0.0002,D_lr:0.0002,time:00h01m30s\n","epoch:[1/30],iter:[131/1611],loss_G:0.9147306680679321,loss_d:0.6227165460586548,G_lr:0.0002,D_lr:0.0002,time:00h01m31s\n","epoch:[1/30],iter:[132/1611],loss_G:0.9571884274482727,loss_d:0.5899285078048706,G_lr:0.0002,D_lr:0.0002,time:00h01m31s\n","epoch:[1/30],iter:[133/1611],loss_G:0.8349756598472595,loss_d:0.5764301419258118,G_lr:0.0002,D_lr:0.0002,time:00h01m32s\n","epoch:[1/30],iter:[134/1611],loss_G:0.9316427111625671,loss_d:0.5699374675750732,G_lr:0.0002,D_lr:0.0002,time:00h01m33s\n","epoch:[1/30],iter:[135/1611],loss_G:0.9570690393447876,loss_d:0.5898493528366089,G_lr:0.0002,D_lr:0.0002,time:00h01m33s\n","epoch:[1/30],iter:[136/1611],loss_G:1.0074433088302612,loss_d:0.6004438400268555,G_lr:0.0002,D_lr:0.0002,time:00h01m34s\n","epoch:[1/30],iter:[137/1611],loss_G:0.8956359028816223,loss_d:0.6055974960327148,G_lr:0.0002,D_lr:0.0002,time:00h01m35s\n","epoch:[1/30],iter:[138/1611],loss_G:0.8848215937614441,loss_d:0.625166654586792,G_lr:0.0002,D_lr:0.0002,time:00h01m35s\n","epoch:[1/30],iter:[139/1611],loss_G:0.8429095149040222,loss_d:0.5541211366653442,G_lr:0.0002,D_lr:0.0002,time:00h01m36s\n","epoch:[1/30],iter:[140/1611],loss_G:0.8969785571098328,loss_d:0.5663927793502808,G_lr:0.0002,D_lr:0.0002,time:00h01m37s\n","epoch:[1/30],iter:[141/1611],loss_G:0.8458843231201172,loss_d:0.6022001504898071,G_lr:0.0002,D_lr:0.0002,time:00h01m37s\n","epoch:[1/30],iter:[142/1611],loss_G:0.8710776567459106,loss_d:0.5890934467315674,G_lr:0.0002,D_lr:0.0002,time:00h01m38s\n","epoch:[1/30],iter:[143/1611],loss_G:0.941935658454895,loss_d:0.6142808198928833,G_lr:0.0002,D_lr:0.0002,time:00h01m39s\n","epoch:[1/30],iter:[144/1611],loss_G:0.8846626281738281,loss_d:0.5782977342605591,G_lr:0.0002,D_lr:0.0002,time:00h01m40s\n","epoch:[1/30],iter:[145/1611],loss_G:0.8646343350410461,loss_d:0.5547417998313904,G_lr:0.0002,D_lr:0.0002,time:00h01m40s\n","epoch:[1/30],iter:[146/1611],loss_G:0.919224202632904,loss_d:0.5493441820144653,G_lr:0.0002,D_lr:0.0002,time:00h01m41s\n","epoch:[1/30],iter:[147/1611],loss_G:0.9354951977729797,loss_d:0.5539951324462891,G_lr:0.0002,D_lr:0.0002,time:00h01m42s\n","epoch:[1/30],iter:[148/1611],loss_G:0.8842740058898926,loss_d:0.5635440349578857,G_lr:0.0002,D_lr:0.0002,time:00h01m42s\n","epoch:[1/30],iter:[149/1611],loss_G:1.1558845043182373,loss_d:0.6706121563911438,G_lr:0.0002,D_lr:0.0002,time:00h01m43s\n","epoch:[1/30],iter:[150/1611],loss_G:0.9203969240188599,loss_d:0.5796637535095215,G_lr:0.0002,D_lr:0.0002,time:00h01m44s\n","epoch:[1/30],iter:[151/1611],loss_G:0.7720507979393005,loss_d:0.584109902381897,G_lr:0.0002,D_lr:0.0002,time:00h01m44s\n","epoch:[1/30],iter:[152/1611],loss_G:1.0538626909255981,loss_d:0.5680698752403259,G_lr:0.0002,D_lr:0.0002,time:00h01m45s\n","epoch:[1/30],iter:[153/1611],loss_G:0.9865137338638306,loss_d:0.670322597026825,G_lr:0.0002,D_lr:0.0002,time:00h01m46s\n","epoch:[1/30],iter:[154/1611],loss_G:0.9235868453979492,loss_d:0.5976312160491943,G_lr:0.0002,D_lr:0.0002,time:00h01m46s\n","epoch:[1/30],iter:[155/1611],loss_G:0.7882384061813354,loss_d:0.568386435508728,G_lr:0.0002,D_lr:0.0002,time:00h01m47s\n","epoch:[1/30],iter:[156/1611],loss_G:0.9495803117752075,loss_d:0.5709115862846375,G_lr:0.0002,D_lr:0.0002,time:00h01m48s\n","epoch:[1/30],iter:[157/1611],loss_G:0.8824374079704285,loss_d:0.6127617359161377,G_lr:0.0002,D_lr:0.0002,time:00h01m48s\n","epoch:[1/30],iter:[158/1611],loss_G:0.8445483446121216,loss_d:0.5581363439559937,G_lr:0.0002,D_lr:0.0002,time:00h01m49s\n","epoch:[1/30],iter:[159/1611],loss_G:0.8908354640007019,loss_d:0.527661919593811,G_lr:0.0002,D_lr:0.0002,time:00h01m50s\n","epoch:[1/30],iter:[160/1611],loss_G:0.9253241419792175,loss_d:0.5899553894996643,G_lr:0.0002,D_lr:0.0002,time:00h01m50s\n","epoch:[1/30],iter:[161/1611],loss_G:0.9699331521987915,loss_d:0.5804346799850464,G_lr:0.0002,D_lr:0.0002,time:00h01m51s\n","epoch:[1/30],iter:[162/1611],loss_G:1.0195549726486206,loss_d:0.6638299226760864,G_lr:0.0002,D_lr:0.0002,time:00h01m52s\n","epoch:[1/30],iter:[163/1611],loss_G:0.9622839093208313,loss_d:0.602453351020813,G_lr:0.0002,D_lr:0.0002,time:00h01m52s\n","epoch:[1/30],iter:[164/1611],loss_G:0.8186655044555664,loss_d:0.5722125172615051,G_lr:0.0002,D_lr:0.0002,time:00h01m53s\n","epoch:[1/30],iter:[165/1611],loss_G:0.9047342538833618,loss_d:0.5707677602767944,G_lr:0.0002,D_lr:0.0002,time:00h01m54s\n","epoch:[1/30],iter:[166/1611],loss_G:1.0502320528030396,loss_d:0.5573904514312744,G_lr:0.0002,D_lr:0.0002,time:00h01m55s\n","epoch:[1/30],iter:[167/1611],loss_G:0.9835659265518188,loss_d:0.5939284563064575,G_lr:0.0002,D_lr:0.0002,time:00h01m55s\n","epoch:[1/30],iter:[168/1611],loss_G:0.7932385206222534,loss_d:0.594543993473053,G_lr:0.0002,D_lr:0.0002,time:00h01m56s\n","epoch:[1/30],iter:[169/1611],loss_G:0.7807977795600891,loss_d:0.5179399847984314,G_lr:0.0002,D_lr:0.0002,time:00h01m57s\n","epoch:[1/30],iter:[170/1611],loss_G:1.008283257484436,loss_d:0.5768260955810547,G_lr:0.0002,D_lr:0.0002,time:00h01m57s\n","epoch:[1/30],iter:[171/1611],loss_G:0.8915185332298279,loss_d:0.5865694880485535,G_lr:0.0002,D_lr:0.0002,time:00h01m58s\n","epoch:[1/30],iter:[172/1611],loss_G:0.8435816168785095,loss_d:0.5648993253707886,G_lr:0.0002,D_lr:0.0002,time:00h01m59s\n","epoch:[1/30],iter:[173/1611],loss_G:0.934741199016571,loss_d:0.5215097069740295,G_lr:0.0002,D_lr:0.0002,time:00h01m59s\n","epoch:[1/30],iter:[174/1611],loss_G:1.03740656375885,loss_d:0.6253421306610107,G_lr:0.0002,D_lr:0.0002,time:00h02m00s\n","epoch:[1/30],iter:[175/1611],loss_G:1.0613716840744019,loss_d:0.598626971244812,G_lr:0.0002,D_lr:0.0002,time:00h02m01s\n","epoch:[1/30],iter:[176/1611],loss_G:0.7347794771194458,loss_d:0.610133171081543,G_lr:0.0002,D_lr:0.0002,time:00h02m01s\n","epoch:[1/30],iter:[177/1611],loss_G:0.8184472918510437,loss_d:0.5905423760414124,G_lr:0.0002,D_lr:0.0002,time:00h02m02s\n","epoch:[1/30],iter:[178/1611],loss_G:0.9334712028503418,loss_d:0.5635298490524292,G_lr:0.0002,D_lr:0.0002,time:00h02m03s\n","epoch:[1/30],iter:[179/1611],loss_G:0.8261009454727173,loss_d:0.5630557537078857,G_lr:0.0002,D_lr:0.0002,time:00h02m03s\n","epoch:[1/30],iter:[180/1611],loss_G:0.9796342253684998,loss_d:0.545056164264679,G_lr:0.0002,D_lr:0.0002,time:00h02m04s\n","epoch:[1/30],iter:[181/1611],loss_G:0.8594865798950195,loss_d:0.5931459665298462,G_lr:0.0002,D_lr:0.0002,time:00h02m05s\n","epoch:[1/30],iter:[182/1611],loss_G:0.8558372855186462,loss_d:0.6124305725097656,G_lr:0.0002,D_lr:0.0002,time:00h02m05s\n","epoch:[1/30],iter:[183/1611],loss_G:0.889578640460968,loss_d:0.6152794361114502,G_lr:0.0002,D_lr:0.0002,time:00h02m06s\n","epoch:[1/30],iter:[184/1611],loss_G:0.9093121886253357,loss_d:0.5624300837516785,G_lr:0.0002,D_lr:0.0002,time:00h02m07s\n","epoch:[1/30],iter:[185/1611],loss_G:0.823132336139679,loss_d:0.6044063568115234,G_lr:0.0002,D_lr:0.0002,time:00h02m08s\n","epoch:[1/30],iter:[186/1611],loss_G:0.9118949174880981,loss_d:0.5553573369979858,G_lr:0.0002,D_lr:0.0002,time:00h02m08s\n","epoch:[1/30],iter:[187/1611],loss_G:1.0524858236312866,loss_d:0.6591956615447998,G_lr:0.0002,D_lr:0.0002,time:00h02m09s\n","epoch:[1/30],iter:[188/1611],loss_G:1.0975157022476196,loss_d:0.5846500396728516,G_lr:0.0002,D_lr:0.0002,time:00h02m10s\n","epoch:[1/30],iter:[189/1611],loss_G:0.8401361703872681,loss_d:0.6302708387374878,G_lr:0.0002,D_lr:0.0002,time:00h02m10s\n","epoch:[1/30],iter:[190/1611],loss_G:0.7784512639045715,loss_d:0.5522104501724243,G_lr:0.0002,D_lr:0.0002,time:00h02m11s\n","epoch:[1/30],iter:[191/1611],loss_G:0.9430482983589172,loss_d:0.5620701313018799,G_lr:0.0002,D_lr:0.0002,time:00h02m12s\n","epoch:[1/30],iter:[192/1611],loss_G:0.926062285900116,loss_d:0.5711742639541626,G_lr:0.0002,D_lr:0.0002,time:00h02m12s\n","epoch:[1/30],iter:[193/1611],loss_G:0.8512198328971863,loss_d:0.551310658454895,G_lr:0.0002,D_lr:0.0002,time:00h02m13s\n","epoch:[1/30],iter:[194/1611],loss_G:0.9314205050468445,loss_d:0.569749653339386,G_lr:0.0002,D_lr:0.0002,time:00h02m14s\n","epoch:[1/30],iter:[195/1611],loss_G:0.9196990132331848,loss_d:0.590582013130188,G_lr:0.0002,D_lr:0.0002,time:00h02m14s\n","epoch:[1/30],iter:[196/1611],loss_G:0.906804084777832,loss_d:0.5335707664489746,G_lr:0.0002,D_lr:0.0002,time:00h02m15s\n","epoch:[1/30],iter:[197/1611],loss_G:0.8636897206306458,loss_d:0.5188086032867432,G_lr:0.0002,D_lr:0.0002,time:00h02m16s\n","epoch:[1/30],iter:[198/1611],loss_G:1.11160147190094,loss_d:0.6498361825942993,G_lr:0.0002,D_lr:0.0002,time:00h02m16s\n","epoch:[1/30],iter:[199/1611],loss_G:1.2167054414749146,loss_d:0.6017842292785645,G_lr:0.0002,D_lr:0.0002,time:00h02m17s\n","epoch:[1/30],iter:[200/1611],loss_G:0.8259067535400391,loss_d:0.5719658136367798,G_lr:0.0002,D_lr:0.0002,time:00h02m18s\n","epoch:[1/30],iter:[201/1611],loss_G:0.801925003528595,loss_d:0.6434234976768494,G_lr:0.0002,D_lr:0.0002,time:00h02m18s\n","epoch:[1/30],iter:[202/1611],loss_G:0.9998152852058411,loss_d:0.4957023859024048,G_lr:0.0002,D_lr:0.0002,time:00h02m19s\n","epoch:[1/30],iter:[203/1611],loss_G:0.9035503268241882,loss_d:0.6641817092895508,G_lr:0.0002,D_lr:0.0002,time:00h02m20s\n","epoch:[1/30],iter:[204/1611],loss_G:0.7528603672981262,loss_d:0.5799511671066284,G_lr:0.0002,D_lr:0.0002,time:00h02m20s\n","epoch:[1/30],iter:[205/1611],loss_G:0.9425337910652161,loss_d:0.5657355785369873,G_lr:0.0002,D_lr:0.0002,time:00h02m21s\n","epoch:[1/30],iter:[206/1611],loss_G:0.9529746174812317,loss_d:0.5268241167068481,G_lr:0.0002,D_lr:0.0002,time:00h02m22s\n","epoch:[1/30],iter:[207/1611],loss_G:0.9498559832572937,loss_d:0.5494852662086487,G_lr:0.0002,D_lr:0.0002,time:00h02m23s\n","epoch:[1/30],iter:[208/1611],loss_G:0.9489332437515259,loss_d:0.5382172465324402,G_lr:0.0002,D_lr:0.0002,time:00h02m23s\n","epoch:[1/30],iter:[209/1611],loss_G:1.3105278015136719,loss_d:0.6604398488998413,G_lr:0.0002,D_lr:0.0002,time:00h02m24s\n","epoch:[1/30],iter:[210/1611],loss_G:1.4190672636032104,loss_d:0.7034597396850586,G_lr:0.0002,D_lr:0.0002,time:00h02m25s\n","epoch:[1/30],iter:[211/1611],loss_G:1.4521524906158447,loss_d:0.7315346002578735,G_lr:0.0002,D_lr:0.0002,time:00h02m25s\n","epoch:[1/30],iter:[212/1611],loss_G:1.2042019367218018,loss_d:0.6985793113708496,G_lr:0.0002,D_lr:0.0002,time:00h02m26s\n","epoch:[1/30],iter:[213/1611],loss_G:0.6844035983085632,loss_d:0.6146280765533447,G_lr:0.0002,D_lr:0.0002,time:00h02m27s\n","epoch:[1/30],iter:[214/1611],loss_G:1.0698535442352295,loss_d:0.593371570110321,G_lr:0.0002,D_lr:0.0002,time:00h02m27s\n","epoch:[1/30],iter:[215/1611],loss_G:1.0347164869308472,loss_d:0.5938899517059326,G_lr:0.0002,D_lr:0.0002,time:00h02m28s\n","epoch:[1/30],iter:[216/1611],loss_G:0.7902712225914001,loss_d:0.5849343538284302,G_lr:0.0002,D_lr:0.0002,time:00h02m29s\n","epoch:[1/30],iter:[217/1611],loss_G:0.7654096484184265,loss_d:0.598156213760376,G_lr:0.0002,D_lr:0.0002,time:00h02m29s\n","epoch:[1/30],iter:[218/1611],loss_G:0.9256227612495422,loss_d:0.5564746856689453,G_lr:0.0002,D_lr:0.0002,time:00h02m30s\n","epoch:[1/30],iter:[219/1611],loss_G:0.9493502974510193,loss_d:0.5268226265907288,G_lr:0.0002,D_lr:0.0002,time:00h02m31s\n","epoch:[1/30],iter:[220/1611],loss_G:0.9333036541938782,loss_d:0.6049128770828247,G_lr:0.0002,D_lr:0.0002,time:00h02m32s\n","epoch:[1/30],iter:[221/1611],loss_G:1.044703483581543,loss_d:0.5201961994171143,G_lr:0.0002,D_lr:0.0002,time:00h02m32s\n","epoch:[1/30],iter:[222/1611],loss_G:0.907632052898407,loss_d:0.5815366506576538,G_lr:0.0002,D_lr:0.0002,time:00h02m33s\n","epoch:[1/30],iter:[223/1611],loss_G:0.8079125285148621,loss_d:0.5330080389976501,G_lr:0.0002,D_lr:0.0002,time:00h02m34s\n","epoch:[1/30],iter:[224/1611],loss_G:0.8936008810997009,loss_d:0.4987790286540985,G_lr:0.0002,D_lr:0.0002,time:00h02m34s\n","epoch:[1/30],iter:[225/1611],loss_G:0.9242421388626099,loss_d:0.5366620421409607,G_lr:0.0002,D_lr:0.0002,time:00h02m35s\n","epoch:[1/30],iter:[226/1611],loss_G:0.9010034799575806,loss_d:0.5733991861343384,G_lr:0.0002,D_lr:0.0002,time:00h02m36s\n","epoch:[1/30],iter:[227/1611],loss_G:0.9925112724304199,loss_d:0.6884739995002747,G_lr:0.0002,D_lr:0.0002,time:00h02m36s\n","epoch:[1/30],iter:[228/1611],loss_G:0.9021092653274536,loss_d:0.568746030330658,G_lr:0.0002,D_lr:0.0002,time:00h02m37s\n","epoch:[1/30],iter:[229/1611],loss_G:0.7881067991256714,loss_d:0.5363646149635315,G_lr:0.0002,D_lr:0.0002,time:00h02m38s\n","epoch:[1/30],iter:[230/1611],loss_G:1.0982286930084229,loss_d:0.6545431613922119,G_lr:0.0002,D_lr:0.0002,time:00h02m38s\n","epoch:[1/30],iter:[231/1611],loss_G:1.0727157592773438,loss_d:0.593370795249939,G_lr:0.0002,D_lr:0.0002,time:00h02m39s\n","epoch:[1/30],iter:[232/1611],loss_G:0.8517473340034485,loss_d:0.5837796926498413,G_lr:0.0002,D_lr:0.0002,time:00h02m40s\n","epoch:[1/30],iter:[233/1611],loss_G:0.9680159687995911,loss_d:0.5326551198959351,G_lr:0.0002,D_lr:0.0002,time:00h02m40s\n","epoch:[1/30],iter:[234/1611],loss_G:0.8710575103759766,loss_d:0.6040588021278381,G_lr:0.0002,D_lr:0.0002,time:00h02m41s\n","epoch:[1/30],iter:[235/1611],loss_G:0.9547152519226074,loss_d:0.6204822659492493,G_lr:0.0002,D_lr:0.0002,time:00h02m42s\n","epoch:[1/30],iter:[236/1611],loss_G:0.9449292421340942,loss_d:0.5585219264030457,G_lr:0.0002,D_lr:0.0002,time:00h02m43s\n","epoch:[1/30],iter:[237/1611],loss_G:0.7960516810417175,loss_d:0.5799741744995117,G_lr:0.0002,D_lr:0.0002,time:00h02m43s\n","epoch:[1/30],iter:[238/1611],loss_G:0.7526113986968994,loss_d:0.5402782559394836,G_lr:0.0002,D_lr:0.0002,time:00h02m44s\n","epoch:[1/30],iter:[239/1611],loss_G:1.0056902170181274,loss_d:0.5438888072967529,G_lr:0.0002,D_lr:0.0002,time:00h02m45s\n","epoch:[1/30],iter:[240/1611],loss_G:0.9023558497428894,loss_d:0.59248948097229,G_lr:0.0002,D_lr:0.0002,time:00h02m45s\n","epoch:[1/30],iter:[241/1611],loss_G:0.796057403087616,loss_d:0.5610450506210327,G_lr:0.0002,D_lr:0.0002,time:00h02m46s\n","epoch:[1/30],iter:[242/1611],loss_G:0.9923948645591736,loss_d:0.6478773355484009,G_lr:0.0002,D_lr:0.0002,time:00h02m47s\n","epoch:[1/30],iter:[243/1611],loss_G:1.034196376800537,loss_d:0.6060358285903931,G_lr:0.0002,D_lr:0.0002,time:00h02m47s\n","epoch:[1/30],iter:[244/1611],loss_G:0.8603043556213379,loss_d:0.5894198417663574,G_lr:0.0002,D_lr:0.0002,time:00h02m48s\n","epoch:[1/30],iter:[245/1611],loss_G:0.821782112121582,loss_d:0.571558952331543,G_lr:0.0002,D_lr:0.0002,time:00h02m49s\n","epoch:[1/30],iter:[246/1611],loss_G:0.9379767775535583,loss_d:0.5158271193504333,G_lr:0.0002,D_lr:0.0002,time:00h02m49s\n","epoch:[1/30],iter:[247/1611],loss_G:0.9140278697013855,loss_d:0.5204218029975891,G_lr:0.0002,D_lr:0.0002,time:00h02m50s\n","epoch:[1/30],iter:[248/1611],loss_G:0.8763805627822876,loss_d:0.5681986212730408,G_lr:0.0002,D_lr:0.0002,time:00h02m51s\n","epoch:[1/30],iter:[249/1611],loss_G:0.9419081807136536,loss_d:0.6214443445205688,G_lr:0.0002,D_lr:0.0002,time:00h02m51s\n","epoch:[1/30],iter:[250/1611],loss_G:0.9129030108451843,loss_d:0.5389242768287659,G_lr:0.0002,D_lr:0.0002,time:00h02m52s\n","epoch:[1/30],iter:[251/1611],loss_G:0.9307618141174316,loss_d:0.5945097208023071,G_lr:0.0002,D_lr:0.0002,time:00h02m53s\n","epoch:[1/30],iter:[252/1611],loss_G:0.9649556279182434,loss_d:0.5655041933059692,G_lr:0.0002,D_lr:0.0002,time:00h02m54s\n","epoch:[1/30],iter:[253/1611],loss_G:0.8106480836868286,loss_d:0.5603034496307373,G_lr:0.0002,D_lr:0.0002,time:00h02m54s\n","epoch:[1/30],iter:[254/1611],loss_G:0.8445583581924438,loss_d:0.5475265383720398,G_lr:0.0002,D_lr:0.0002,time:00h02m55s\n","epoch:[1/30],iter:[255/1611],loss_G:0.8883302807807922,loss_d:0.5438585877418518,G_lr:0.0002,D_lr:0.0002,time:00h02m56s\n","epoch:[1/30],iter:[256/1611],loss_G:0.8981682062149048,loss_d:0.5369290113449097,G_lr:0.0002,D_lr:0.0002,time:00h02m56s\n","epoch:[1/30],iter:[257/1611],loss_G:0.8842092752456665,loss_d:0.562311589717865,G_lr:0.0002,D_lr:0.0002,time:00h02m57s\n","epoch:[1/30],iter:[258/1611],loss_G:0.9280697107315063,loss_d:0.5665783882141113,G_lr:0.0002,D_lr:0.0002,time:00h02m58s\n","epoch:[1/30],iter:[259/1611],loss_G:0.9023521542549133,loss_d:0.5501936674118042,G_lr:0.0002,D_lr:0.0002,time:00h02m58s\n","epoch:[1/30],iter:[260/1611],loss_G:0.8322615623474121,loss_d:0.525810718536377,G_lr:0.0002,D_lr:0.0002,time:00h02m59s\n","epoch:[1/30],iter:[261/1611],loss_G:0.8358546495437622,loss_d:0.48163408041000366,G_lr:0.0002,D_lr:0.0002,time:00h03m00s\n","epoch:[1/30],iter:[262/1611],loss_G:1.0980032682418823,loss_d:0.535965085029602,G_lr:0.0002,D_lr:0.0002,time:00h03m00s\n","epoch:[1/30],iter:[263/1611],loss_G:0.9300475120544434,loss_d:0.600055992603302,G_lr:0.0002,D_lr:0.0002,time:00h03m01s\n","epoch:[1/30],iter:[264/1611],loss_G:0.8051505088806152,loss_d:0.5815551280975342,G_lr:0.0002,D_lr:0.0002,time:00h03m02s\n","epoch:[1/30],iter:[265/1611],loss_G:0.7914352416992188,loss_d:0.5703678727149963,G_lr:0.0002,D_lr:0.0002,time:00h03m02s\n","epoch:[1/30],iter:[266/1611],loss_G:1.0353811979293823,loss_d:0.627332329750061,G_lr:0.0002,D_lr:0.0002,time:00h03m03s\n","epoch:[1/30],iter:[267/1611],loss_G:1.0037645101547241,loss_d:0.6177574396133423,G_lr:0.0002,D_lr:0.0002,time:00h03m04s\n","epoch:[1/30],iter:[268/1611],loss_G:0.8137457966804504,loss_d:0.5431427955627441,G_lr:0.0002,D_lr:0.0002,time:00h03m05s\n","epoch:[1/30],iter:[269/1611],loss_G:1.04383385181427,loss_d:0.5563596487045288,G_lr:0.0002,D_lr:0.0002,time:00h03m05s\n","epoch:[1/30],iter:[270/1611],loss_G:0.9641297459602356,loss_d:0.561969518661499,G_lr:0.0002,D_lr:0.0002,time:00h03m06s\n","epoch:[1/30],iter:[271/1611],loss_G:0.8148250579833984,loss_d:0.5371785163879395,G_lr:0.0002,D_lr:0.0002,time:00h03m07s\n","epoch:[1/30],iter:[272/1611],loss_G:0.942505955696106,loss_d:0.5256116390228271,G_lr:0.0002,D_lr:0.0002,time:00h03m07s\n","epoch:[1/30],iter:[273/1611],loss_G:1.0221891403198242,loss_d:0.5977292060852051,G_lr:0.0002,D_lr:0.0002,time:00h03m08s\n","epoch:[1/30],iter:[274/1611],loss_G:0.9571842551231384,loss_d:0.5635665655136108,G_lr:0.0002,D_lr:0.0002,time:00h03m09s\n","epoch:[1/30],iter:[275/1611],loss_G:0.8222449421882629,loss_d:0.5448461771011353,G_lr:0.0002,D_lr:0.0002,time:00h03m09s\n","epoch:[1/30],iter:[276/1611],loss_G:0.9347996115684509,loss_d:0.5199441909790039,G_lr:0.0002,D_lr:0.0002,time:00h03m10s\n","epoch:[1/30],iter:[277/1611],loss_G:0.8162904977798462,loss_d:0.5293495059013367,G_lr:0.0002,D_lr:0.0002,time:00h03m11s\n","epoch:[1/30],iter:[278/1611],loss_G:0.910319983959198,loss_d:0.5172052979469299,G_lr:0.0002,D_lr:0.0002,time:00h03m11s\n","epoch:[1/30],iter:[279/1611],loss_G:1.0683873891830444,loss_d:0.5649417042732239,G_lr:0.0002,D_lr:0.0002,time:00h03m12s\n","epoch:[1/30],iter:[280/1611],loss_G:0.8669546246528625,loss_d:0.533961296081543,G_lr:0.0002,D_lr:0.0002,time:00h03m13s\n","epoch:[1/30],iter:[281/1611],loss_G:0.7565214037895203,loss_d:0.5325973033905029,G_lr:0.0002,D_lr:0.0002,time:00h03m13s\n","epoch:[1/30],iter:[282/1611],loss_G:0.9442374110221863,loss_d:0.551570475101471,G_lr:0.0002,D_lr:0.0002,time:00h03m14s\n","epoch:[1/30],iter:[283/1611],loss_G:0.8896255493164062,loss_d:0.5008224844932556,G_lr:0.0002,D_lr:0.0002,time:00h03m15s\n","epoch:[1/30],iter:[284/1611],loss_G:1.0906747579574585,loss_d:0.5454535484313965,G_lr:0.0002,D_lr:0.0002,time:00h03m15s\n"]}],"source":["import argparse\n","import os\n","def parse_args():\n","    desc='pytorch of My-CartoonGAN'\n","    parser=argparse.ArgumentParser(desc)\n","    parser.add_argument('--device',type=str,default='cuda',choices=['cuda','cpu'])\n","    parser.add_argument('--input_c', type=int, default=3)\n","    parser.add_argument('--epoch', type=int, default=150)\n","    parser.add_argument('--pre_epoch', type=int, default=20)\n","    parser.add_argument('--cpu_count', type=int, default=10)\n","    parser.add_argument('--init_lr',type=float,default=0.002)\n","    parser.add_argument('--g_lr', type=float, default=0.0002, help='learning rate for ADAM')\n","    parser.add_argument('--d_lr', type=float, default=0.0002, help='learning rate for ADAM')\n","    parser.add_argument('--decay_lr', type=float, default=0.0002, help='learning rate for ADAM')#3.000000000000006e-06\n","    parser.add_argument('--hw', type=int, default=256)\n","    parser.add_argument('--result_dir',type=str,default='results')\n","    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',\n","                        help='Name of checkpoint directory')\n","    parser.add_argument('--dataset', type=str, default='hayao')\n","    parser.add_argument('--data_dir',type=str,default='your data')\n","    parser.add_argument('--test_dir',type=str,default='your weight')\n","    parser.add_argument('--isTrain',type=bool,default=True)  #Train or Test\n","    parser.add_argument('--retrain', type=bool, default=False)\n","    parser.add_argument('--isTest', type=bool, default=False)\n","    parser.add_argument('--train_init', type=bool, default=False)\n","    parser.add_argument('--b1',type=int,default=0.5)\n","    parser.add_argument('--b2', type=int, default=0.999)\n","    parser.add_argument('--y1', type=int, default=1)\n","    parser.add_argument('--y2', type=int, default=10)\n","    parser.add_argument('--latent_dim', type=int, default=64)\n","    parser.add_argument('--patch_size', type=int, default=96)\n","    parser.add_argument('--s', type=int, default=48)\n","    parser.add_argument('--batch_size',type=int,default=4)\n","    parser.add_argument('--save_pred',type=int,default=1)\n","    parser.add_argument('--weight_content',type=float,default=2)#big\n","    parser.add_argument('--weight_struct',type=float,default=2)#\n","    parser.add_argument('--weight_surface', type=float, default=5)\n","    parser.add_argument('--weight_testure', type=float, default=5)\n","    parser.add_argument('--weight_classifer', type=float, default=0)\n","    parser.add_argument('--weight_tv', type=float, default=1)\n","    parser.add_argument('--weight_style', type=float, default=2)\n","    parser.add_argument('--weight_edge', type=float, default=0.001)\n","    parser.add_argument('--weight_decay', type=float, default=0.0001)\n","    return check_args(parser.parse_args(args=[]))\n","def check_args(args):\n","    check_folder(os.path.join(args.result_dir, args.dataset, 'checkpoint'))\n","    check_folder(os.path.join(args.result_dir, args.dataset, 'img'))\n","    check_folder(os.path.join(args.result_dir, args.dataset, 'test'))\n","    check_folder(os.path.join(args.result_dir, args.dataset, 'test','testA'))\n","    check_folder(os.path.join(args.result_dir, args.dataset, 'test','testB'))\n","    return args\n","def main():\n","   args=parse_args()\n","   gan=MyGAN(args)\n","   if args.isTrain:\n","       if args.retrain:\n","            gan.load_model()\n","       print(f\"training on {args.device}\")\n","       gan.train()\n","       print(\"train haved finished\")\n","   if args.isTest:\n","       gan.test()\n","       print(\"test haved finished\")\n","if __name__==\"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import zipfile\n","def file2zip(packagePath, zipPath):\n","    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n","    for path, dirNames, fileNames in os.walk(packagePath):\n","        fpath = path.replace(packagePath, '')\n","        for name in fileNames:\n","            fullName = os.path.join(path, name)\n","            name = fpath + '\\\\' + name\n","            zip.write(fullName, name)\n","    zip.close()\n","if __name__ == \"__main__\":\n","    # 文件夹路径\n","    packagePath = 'results/hayao/img'\n","    zipPath = 'rcct50.zip' \n","    if os.path.exists(zipPath):\n","        os.remove(zipPath)\n","    file2zip(packagePath, zipPath)\n","    print(\"打包完成\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2931213,"sourceId":5048958,"sourceType":"datasetVersion"},{"datasetId":3174200,"sourceId":5502201,"sourceType":"datasetVersion"},{"datasetId":3181357,"sourceId":5516853,"sourceType":"datasetVersion"},{"datasetId":3222064,"sourceId":5601051,"sourceType":"datasetVersion"},{"datasetId":4077373,"sourceId":7078478,"sourceType":"datasetVersion"},{"datasetId":4080231,"sourceId":7082503,"sourceType":"datasetVersion"},{"datasetId":4284395,"sourceId":7373718,"sourceType":"datasetVersion"},{"datasetId":4569809,"sourceId":7803918,"sourceType":"datasetVersion"},{"datasetId":4572903,"sourceId":7808125,"sourceType":"datasetVersion"},{"datasetId":4575627,"sourceId":7811914,"sourceType":"datasetVersion"},{"datasetId":4575637,"sourceId":7811928,"sourceType":"datasetVersion"},{"datasetId":4577123,"sourceId":7813839,"sourceType":"datasetVersion"},{"datasetId":4581535,"sourceId":7819847,"sourceType":"datasetVersion"},{"datasetId":4293391,"sourceId":7386530,"sourceType":"datasetVersion"},{"datasetId":4293400,"sourceId":7386540,"sourceType":"datasetVersion"},{"datasetId":4583275,"sourceId":7822307,"sourceType":"datasetVersion"},{"datasetId":2679631,"sourceId":4600373,"sourceType":"datasetVersion"},{"datasetId":3170169,"sourceId":5493573,"sourceType":"datasetVersion"},{"datasetId":3176066,"sourceId":5505821,"sourceType":"datasetVersion"},{"datasetId":3189932,"sourceId":5534212,"sourceType":"datasetVersion"},{"datasetId":4553370,"sourceId":7781061,"sourceType":"datasetVersion"},{"datasetId":4558750,"sourceId":7788524,"sourceType":"datasetVersion"},{"datasetId":4566910,"sourceId":7799803,"sourceType":"datasetVersion"},{"datasetId":4572678,"sourceId":7807815,"sourceType":"datasetVersion"},{"datasetId":4572709,"sourceId":7807852,"sourceType":"datasetVersion"},{"datasetId":4607596,"sourceId":7855791,"sourceType":"datasetVersion"},{"datasetId":4655891,"sourceId":7922554,"sourceType":"datasetVersion"},{"datasetId":4658403,"sourceId":7926372,"sourceType":"datasetVersion"},{"datasetId":4695410,"sourceId":7978190,"sourceType":"datasetVersion"},{"datasetId":4698016,"sourceId":7981880,"sourceType":"datasetVersion"},{"datasetId":4703915,"sourceId":7990366,"sourceType":"datasetVersion"},{"datasetId":4706107,"sourceId":7993664,"sourceType":"datasetVersion"},{"datasetId":4717338,"sourceId":8009070,"sourceType":"datasetVersion"},{"datasetId":4721774,"sourceId":8014559,"sourceType":"datasetVersion"},{"datasetId":4721784,"sourceId":8014580,"sourceType":"datasetVersion"},{"datasetId":4726031,"sourceId":8020444,"sourceType":"datasetVersion"},{"datasetId":4594826,"sourceId":7838379,"sourceType":"datasetVersion"},{"datasetId":4594851,"sourceId":7838406,"sourceType":"datasetVersion"},{"datasetId":5080155,"sourceId":8510478,"sourceType":"datasetVersion"},{"datasetId":5082979,"sourceId":8514209,"sourceType":"datasetVersion"},{"datasetId":3182435,"sourceId":5518451,"sourceType":"datasetVersion"},{"datasetId":3218010,"sourceId":5593324,"sourceType":"datasetVersion"},{"datasetId":4581511,"sourceId":7819796,"sourceType":"datasetVersion"},{"datasetId":4603550,"sourceId":7850239,"sourceType":"datasetVersion"},{"datasetId":5088882,"sourceId":8522538,"sourceType":"datasetVersion"},{"datasetId":5091181,"sourceId":8525796,"sourceType":"datasetVersion"},{"datasetId":5095237,"sourceId":8531269,"sourceType":"datasetVersion"},{"datasetId":5098183,"sourceId":8535344,"sourceType":"datasetVersion"},{"datasetId":5101913,"sourceId":8540262,"sourceType":"datasetVersion"},{"datasetId":5112039,"sourceId":8554291,"sourceType":"datasetVersion"},{"datasetId":5116613,"sourceId":8560369,"sourceType":"datasetVersion"},{"datasetId":5127032,"sourceId":8574192,"sourceType":"datasetVersion"},{"datasetId":5129837,"sourceId":8578163,"sourceType":"datasetVersion"},{"datasetId":3192321,"sourceId":5538859,"sourceType":"datasetVersion"},{"datasetId":3192327,"sourceId":5538877,"sourceType":"datasetVersion"},{"datasetId":5135320,"sourceId":8585943,"sourceType":"datasetVersion"},{"datasetId":5137908,"sourceId":8589864,"sourceType":"datasetVersion"},{"datasetId":5141890,"sourceId":8595195,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
